{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from torchvision.datasets import MNIST\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集/模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP + MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "optConfig = {\n",
    "    'honestSize': 50,\n",
    "    'byzantineSize': 20,\n",
    "\n",
    "    'rounds': 15,\n",
    "    'displayInterval': 1000,\n",
    "\n",
    "    'weight_decay': 0.00,\n",
    "    \n",
    "    'fixSeed': False,\n",
    "    'SEED': 100,\n",
    "    \n",
    "    'batchSize': 5,\n",
    "    'shuffle': True,\n",
    "}\n",
    "\n",
    "# 数据集属性\n",
    "dataSetConfig = {\n",
    "    'name': 'mnist',\n",
    "\n",
    "    'dataSet' : 'mnist',\n",
    "    'dataSetSize': 60000,\n",
    "    'maxFeature': 784,\n",
    "\n",
    "    'honestNodeSize': 50,\n",
    "    'byzantineNodeSize': 20,\n",
    "\n",
    "    'rounds': 15,\n",
    "    'displayInterval': 1000,\n",
    "}\n",
    "\n",
    "SGDConfig = optConfig.copy()\n",
    "SGDConfig['gamma'] = 1e-1\n",
    "\n",
    "batchConfig = optConfig.copy()\n",
    "batchConfig['batchSize'] = 50\n",
    "batchConfig['gamma'] = 5e-1\n",
    "\n",
    "SVRGConfig = optConfig.copy()\n",
    "SVRGConfig['snapshotInterval'] = dataSetConfig['dataSetSize']\n",
    "SVRGConfig['gamma'] = 1e-1\n",
    "\n",
    "SAGAConfig = optConfig.copy()\n",
    "SAGAConfig['gamma'] = 1e-1\n",
    "\n",
    "SARAHConfig = optConfig.copy()\n",
    "SARAHConfig['gamma'] = 1e-1\n",
    "\n",
    "# 加载数据集\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Convert a PIL Image or numpy.ndarray to tensor.\n",
    "    # Normalize a tensor image with mean 0.1307 and standard deviation 0.3081\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./dataset/', \n",
    "                            train=True, \n",
    "                            transform=train_transform,\n",
    "                            download=True)\n",
    "validate_dataset = torchvision.datasets.MNIST(root='./dataset/', \n",
    "                           train=False, \n",
    "                           transform=test_transform,\n",
    "                           download=False)\n",
    "\n",
    "# 模型\n",
    "class MLP(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs                Linear/Function        Output\n",
    "    [128, 1, 28, 28]   -> Linear(28*28, 100) -> [128, 100]  # first hidden layer\n",
    "                       -> Tanh               -> [128, 100]  # Tanh activation function, may sigmoid\n",
    "                       -> Linear(100, 100)   -> [128, 100]  # third hidden layer\n",
    "                       -> Tanh               -> [128, 100]  # Tanh activation function, may sigmoid\n",
    "                       -> Linear(100, 10)    -> [128, 10]   # Classification Layer                                                          \n",
    "   \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, SEED=100):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.classification_layer = torch.nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.tanh1 = torch.nn.Tanh()\n",
    "        self.tanh2 = torch.nn.Tanh()\n",
    "        \n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Defines the computation performed at every call.\n",
    "           Should be overridden by all subclasses.\n",
    "        Args:\n",
    "            x: [batch_size, channel, height, width], input for network\n",
    "        Returns:\n",
    "            out: [batch_size, n_classes], output from network\n",
    "        \"\"\"\n",
    "        \n",
    "        out = x.view(x.size(0), -1) # flatten x in [128, 784]\n",
    "        out = self.tanh1(out)\n",
    "        out = self.hidden(out)\n",
    "        out = self.tanh2(out)\n",
    "        out = self.classification_layer(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "    \n",
    "# 模型工厂\n",
    "def modelFactory(SEED=100):\n",
    "    return MLP(784, 50, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet + CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet50 + CIFAR10\n",
    "# optConfig = {\n",
    "#     'honestSize': 10,\n",
    "#     'byzantineSize': 4,\n",
    "\n",
    "#     'rounds': 15,\n",
    "#     'displayInterval': 6000,\n",
    "    \n",
    "#     'weight_decay': 0.0001,\n",
    "    \n",
    "#     'fixSeed': False,\n",
    "#     'SEED': 100,\n",
    "    \n",
    "#     'batchSize': 5,\n",
    "#     'shuffle': True,\n",
    "# }\n",
    "\n",
    "# SGDConfig = optConfig.copy()\n",
    "# SGDConfig['gamma'] = 1e-1\n",
    "\n",
    "# batchConfig = optConfig.copy()\n",
    "# batchConfig['batchSize'] = 50\n",
    "# batchConfig['gamma'] = 5e-1\n",
    "\n",
    "# SVRGConfig = optConfig.copy()\n",
    "# SVRGConfig['snapshotInterval'] = dataSetConfig['dataSetSize']\n",
    "# SVRGConfig['gamma'] = 1e-1\n",
    "\n",
    "# SAGAConfig = optConfig.copy()\n",
    "# SAGAConfig['gamma'] = 1e-1\n",
    "\n",
    "# SARAHConfig = optConfig.copy()\n",
    "# SARAHConfig['gamma'] = 1e-1\n",
    "\n",
    "# # 数据集属性\n",
    "# dataSetConfig = {\n",
    "#     'name': 'CIFAR-10',\n",
    "\n",
    "#     'dataSet' : 'CIFAR-10',\n",
    "#     'dataSetSize': 60000,\n",
    "#     'maxFeature': 32*32*3,\n",
    "# }\n",
    "\n",
    "# # 加载数据集\n",
    "# preprocess = transforms.Compose([\n",
    "#     transforms.Resize(256),\n",
    "#     transforms.CenterCrop(224),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "# train_dataset = torchvision.datasets.CIFAR10(root='./dataset/',\n",
    "#                                              train=True, \n",
    "#                                              transform=preprocess,\n",
    "#                                              download=False)\n",
    "# validate_dataset = torchvision.datasets.CIFAR10(root='./dataset/',\n",
    "#                                             train=False, \n",
    "#                                             transform=preprocess)\n",
    "\n",
    "# 模型工厂\n",
    "# def modelFactory(SEED=100):\n",
    "#     return torchvision.models.resnet50()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运行参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = './cache/' + dataSetConfig['name'] + '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 报告函数\n",
    "def log(*k, **kw):\n",
    "    timeStamp = time.strftime('[%m-%d %H:%M:%S] ', time.localtime())\n",
    "    print(timeStamp, end='')\n",
    "    print(*k, **kw)\n",
    "def debug(*k, **kw):\n",
    "    timeStamp = time.strftime('[%m-%d %H:%M:%S] (debug)', time.localtime())\n",
    "    print(timeStamp, end='')\n",
    "    print(*k, **kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVarience(w_local, honestSize):\n",
    "    avg = w_local[:honestSize].mean(dim=0)\n",
    "    s = 0\n",
    "    for w in w_local[:honestSize]:\n",
    "        s += (w - avg).norm()**2\n",
    "    s /= honestSize\n",
    "    return s.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAccuracy(model, loader, device):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    total = 0\n",
    "    \n",
    "    for material, targets in loader:\n",
    "        material, targets = material.to(device), targets.to(device)\n",
    "        outputs = model(material)\n",
    "        \n",
    "        l = loss_func(outputs, targets)\n",
    "\n",
    "        loss += l.item() * len(targets)\n",
    "        _, predicted = torch.max(outputs.data, dim=1)\n",
    "        accuracy += (predicted == targets).sum().item()\n",
    "        total += len(targets)\n",
    "    \n",
    "    loss /= total\n",
    "    accuracy /= total\n",
    "    \n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聚合函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(wList):\n",
    "    return torch.mean(wList, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gm(wList):\n",
    "    max_iter = 80\n",
    "    tol = 1e-5\n",
    "    guess = torch.mean(wList, dim=0)\n",
    "    for _ in range(max_iter):\n",
    "        dist_li = torch.norm(wList-guess, dim=1)\n",
    "        for i in range(len(dist_li)):\n",
    "            if dist_li[i] == 0:\n",
    "                dist_li[i] = 1\n",
    "        temp1 = torch.sum(torch.stack([w/d for w, d in zip(wList, dist_li)]), dim=0)\n",
    "        temp2 = torch.sum(1/dist_li)\n",
    "        guess_next = temp1 / temp2\n",
    "        guess_movement = torch.norm(guess - guess_next)\n",
    "        guess = guess_next\n",
    "        if guess_movement <= tol:\n",
    "            break\n",
    "    return guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Krum_(nodeSize, byzantineSize):\n",
    "    honestSize = nodeSize - byzantineSize\n",
    "    dist = torch.zeros(nodeSize, nodeSize, dtype=torch.float32)\n",
    "    def Krum(wList):\n",
    "        for i in range(nodeSize):\n",
    "            for j in range(i, nodeSize):\n",
    "                distance = wList[i].data - wList[j].data\n",
    "                distance = (distance*distance).sum()\n",
    "                dist[i][j] = distance.data\n",
    "                dist[j][i] = distance.data\n",
    "        k = nodeSize - byzantineSize - 2 + 1 # 算上自己和自己的0.00\n",
    "        topv, _ = dist.topk(k=k, dim=1)\n",
    "        sumdist = -topv.sum(dim=1)\n",
    "        resindex = sumdist.topk(1)[1].squeeze()\n",
    "        return wList[resindex]\n",
    "    return Krum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median(wList):\n",
    "    return wList.median(dim=0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(message, byzantineSize):\n",
    "    wList = [torch.cat([p.flatten() for p in parameters]) for parameters in message]\n",
    "    wList.extend([torch.zeros_like(wList[0]) for _ in range(byzantineSize)])\n",
    "    wList = torch.stack(wList)\n",
    "    return wList\n",
    "def unflatten_vector(vector, model):\n",
    "    paraGroup = []\n",
    "    cum = 0\n",
    "    for p in model.parameters():\n",
    "        newP = vector[cum:cum+p.numel()]\n",
    "        paraGroup.append(newP.view_as(p))\n",
    "        cum += p.numel()\n",
    "    return paraGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomSample(dataset, batchSize):\n",
    "    m, t = zip(*random.sample(dataset, batchSize))\n",
    "    material, targets = torch.cat(m), torch.tensor(t)\n",
    "    return material, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPara(module, useString=True):\n",
    "    para = sum([x.nelement() for x in module.parameters()])\n",
    "    if not useString:\n",
    "        return para\n",
    "    elif para >= 2**20:\n",
    "        return '{:.2f}M'.format(para / 2**20)\n",
    "    elif para >= 2**10:\n",
    "        return '{:.2f}K'.format(para / 2**10)\n",
    "    else:\n",
    "        return str(para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "报告函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(r, rounds, displayInterval, trainLoss, trainAccuracy, valLoss, valAccuracy, var=None):\n",
    "    varStr = '' if (var == None) else ' var={:.2e}'.format(var)\n",
    "    log('[{}/{}](interval: {:.0f}) train: loss={:.4f} acc={:.2f} val: loss={:.4f} acc={:.2f}{}'\n",
    "        .format(r, rounds, displayInterval, trainLoss, trainAccuracy, valLoss, valAccuracy, varStr)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## CentralSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def CentralSGD(model, gamma, aggregate, weight_decay, attack=None, \n",
    "          rounds=10, displayInterval=1000, \n",
    "          device='cpu', SEED=100, fixSeed=False, \n",
    "          batchSize=1,\n",
    "          **kw):\n",
    "    if fixSeed:\n",
    "        random.seed(SEED)\n",
    "\n",
    "    # 顺序遍历loader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batchSize, shuffle=False)\n",
    "    validate_loader = torch.utils.data.DataLoader(dataset=validate_dataset, batch_size=batchSize, shuffle=False)\n",
    "\n",
    "    # 随机取样器\n",
    "    randomSampler = lambda dataset: torch.utils.data.sampler.RandomSampler(\n",
    "        dataset, \n",
    "        num_samples=rounds*displayInterval*batchSize, \n",
    "        replacement=True\n",
    "    )\n",
    "    train_random_loaders_splited = [torch.utils.data.DataLoader(\n",
    "        dataset=subset,\n",
    "        batch_size=batchSize, \n",
    "        sampler=randomSampler(subset),\n",
    "    ) for subset in train_dataset_subset]\n",
    "    randomIters = [iter(loader) for loader in train_random_loaders_splited]\n",
    "    \n",
    "    # 求初始误差\n",
    "    trainLoss, trainAccuracy = calculateAccuracy(model, train_loader, device)\n",
    "    valLoss, valAccuracy = calculateAccuracy(model, validate_loader, device)\n",
    "\n",
    "    trainLossPath = [trainLoss]\n",
    "    trainAccPath = [trainAccuracy]\n",
    "    valLossPath = [valLoss]\n",
    "    valAccPath = [valAccuracy]\n",
    "    \n",
    "    report(0, rounds, displayInterval, trainLoss, trainAccuracy, valLoss, valAccuracy)\n",
    "\n",
    "    for r in range(rounds):\n",
    "        model.train()\n",
    "        for k in range(displayInterval):\n",
    "            # 读取数据\n",
    "            material, targets = next(randomIter)\n",
    "            material, targets = material.to(device), targets.to(device)\n",
    "\n",
    "            # 随机梯度\n",
    "            # --------------------\n",
    "            # 预测\n",
    "            outputs = model(material)\n",
    "            loss = loss_func(outputs, targets)\n",
    "            # 反向传播\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # 更新\n",
    "            for para in model.parameters():\n",
    "                para.data.add_(-gamma, para.grad)\n",
    "                para.data.add_(-weight_decay, para)\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        trainLoss, trainAccuracy = calculateAccuracy(model, train_loader, device)\n",
    "        valLoss, valAccuracy = calculateAccuracy(model, validate_loader, device)\n",
    "\n",
    "        trainLossPath.append(trainLoss)\n",
    "        trainAccPath.append(trainAccuracy)\n",
    "        valLossPath.append(valLoss)\n",
    "        valAccPath.append(valAccuracy)\n",
    "\n",
    "        report(r+1, rounds, displayInterval, trainLoss, trainAccuracy, valLoss, valAccuracy)\n",
    "    return model, trainLossPath, trainAccPath, valLossPath, valAccPath, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Central SARAH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def CentralSARAH(model, gamma, aggregate, weight_decay, \n",
    "          snapshotInterval=len(train_dataset),\n",
    "          rounds=10, displayInterval=1000, \n",
    "          device='cpu', SEED=100, fixSeed=False, \n",
    "          batchSize=5,\n",
    "          **kw):\n",
    "    \n",
    "    if fixSeed:\n",
    "        random.seed(SEED)\n",
    "    \n",
    "    # 初始化模型\n",
    "    lastModel = modelFactory(SEED=SEED)\n",
    "    lastModel = lastModel.to(device)\n",
    "\n",
    "    # 随机的停止期限\n",
    "    randomStop = 1\n",
    "    \n",
    "    # 顺序遍历loader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batchSize, shuffle=False)\n",
    "    validate_loader = torch.utils.data.DataLoader(dataset=validate_dataset, batch_size=batchSize, shuffle=False)\n",
    "    \n",
    "    # 随机取样器\n",
    "    randomSampler = torch.utils.data.sampler.RandomSampler(\n",
    "        train_dataset, \n",
    "        num_samples=rounds*displayInterval*batchSize, \n",
    "        replacement=True\n",
    "    )\n",
    "    randomLoader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batchSize, \n",
    "        sampler=randomSampler,\n",
    "    )\n",
    "    randomIter = iter(randomLoader)\n",
    "    \n",
    "    # 求初始误差\n",
    "    trainLoss, trainAccuracy = calculateAccuracy(model, train_loader, device)\n",
    "    valLoss, valAccuracy = calculateAccuracy(model, validate_loader, device)\n",
    "    \n",
    "    trainLossPath = [trainLoss]\n",
    "    trainAccPath = [trainAccuracy]\n",
    "    valLossPath = [valLoss]\n",
    "    valAccPath = [valAccuracy]\n",
    "    \n",
    "    log('[SARAH]初始 train: loss={:.6f} accuracy={:.2f} validation: loss={:.6f} accuracy={:.2f}'\n",
    "        .format(trainLossPath[0], trainAccPath[0], valLossPath[0], valAccPath[0])\n",
    "    )\n",
    "\n",
    "    gradients = [torch.zeros_like(para, requires_grad=False) for para in model.parameters()]\n",
    "    \n",
    "    for r in range(rounds):\n",
    "        for k in range(displayInterval):\n",
    "            # snapshot\n",
    "            if (r*displayInterval + k) % randomStop == 0:\n",
    "                # 清空旧梯度\n",
    "                for grad in gradients:\n",
    "                    grad.zero_()\n",
    "                for material, targets in train_loader:\n",
    "                    material, targets = material.to(device), targets.to(device)\n",
    "                    # 预测\n",
    "                    outputs = model(material)\n",
    "                    loss = loss_func(outputs, targets)\n",
    "                    # 反向传播\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "\n",
    "                    for grad, para in zip(gradients, model.parameters()):\n",
    "                        grad.data.add_(1/len(train_loader), para.grad.data)\n",
    "                for grad, para in zip(gradients, model.parameters()):\n",
    "                    grad.data.add_(weight_decay, para.data)\n",
    "                \n",
    "                # 保存旧结果\n",
    "                for oldPara, newPara in zip(lastModel.parameters(), model.parameters()):\n",
    "                    oldPara.data.copy_(newPara)\n",
    "                # 更新\n",
    "                for para, grad in zip(model.parameters(), gradients):\n",
    "                    para.data.add_(-gamma, grad)\n",
    "                # 指定下一次停止时间\n",
    "                randomStop = random.randint(1, snapshotInterval-1)\n",
    "                \n",
    "            # 更新\n",
    "            # 读取数据\n",
    "            material, targets = next(randomIter)\n",
    "            material, targets = material.to(device), targets.to(device)\n",
    "\n",
    "            # 随机梯度\n",
    "            # --------------------\n",
    "            # 预测\n",
    "            outputs = model(material)\n",
    "            loss = loss_func(outputs, targets)\n",
    "            # 反向传播\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # 修正梯度\n",
    "            # --------------------\n",
    "            # 预测\n",
    "            outputs = lastModel(material)\n",
    "            loss = loss_func(outputs, targets)\n",
    "            # 反向传播\n",
    "            lastModel.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # 更新梯度表\n",
    "            for pi, para in enumerate(model.parameters()):\n",
    "                gradients[pi].data.add_(1, para.grad.data)\n",
    "                gradients[pi].data.add_(weight_decay, para)\n",
    "            for pi, para in enumerate(lastModel.parameters()):\n",
    "                gradients[pi].data.sub_(1, para.grad.data)\n",
    "                gradients[pi].data.sub_(weight_decay, para)\n",
    "\n",
    "            # 保存旧结果\n",
    "            for oldPara, newPara in zip(lastModel.parameters(), model.parameters()):\n",
    "                oldPara.data.copy_(newPara)\n",
    "            # 更新\n",
    "            for para, grad in zip(model.parameters(), gradients):\n",
    "                para.data.add_(-gamma, grad)\n",
    "  \n",
    "        trainLoss, trainAccuracy = calculateAccuracy(model, train_loader, device)\n",
    "        valLoss, valAccuracy = calculateAccuracy(model, validate_loader, device)\n",
    "\n",
    "        trainLossPath.append(trainLoss)\n",
    "        trainAccPath.append(trainAccuracy)\n",
    "        valLossPath.append(valLoss)\n",
    "        valAccPath.append(valAccuracy)\n",
    "        \n",
    "        report(r+1, rounds, displayInterval, trainLoss, trainAccuracy, valLoss, valAccuracy)\n",
    "    return model, trainLossPath, trainAccPath, valLossPath, valAccPath, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def SGD(model, gamma, aggregate, weight_decay, \n",
    "          honestSize=0, byzantineSize=0, attack=None, \n",
    "          rounds=10, displayInterval=1000, \n",
    "          device='cpu', SEED=100, fixSeed=False, \n",
    "          batchSize=5,\n",
    "          **kw):\n",
    "    assert byzantineSize == 0 or attack != None\n",
    "    assert honestSize != 0\n",
    "    \n",
    "    if fixSeed:\n",
    "        random.seed(SEED)\n",
    "\n",
    "    nodeSize = honestSize + byzantineSize\n",
    "\n",
    "    # 数据分片\n",
    "    pieces = [(i*len(train_dataset)) // honestSize for i in range(honestSize+1)]\n",
    "    dataPerNode = [pieces[i+1] - pieces[i] for i in range(honestSize)]\n",
    "\n",
    "    # 回复的消息\n",
    "    message = [\n",
    "        [torch.zeros_like(para, requires_grad=False) for para in model.parameters()]\n",
    "        for _ in range(nodeSize)]  \n",
    "    \n",
    "    # 顺序遍历loader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batchSize, shuffle=False)\n",
    "    validate_loader = torch.utils.data.DataLoader(dataset=validate_dataset, batch_size=batchSize, shuffle=False)\n",
    "    \n",
    "    train_dataset_subset = [torch.utils.data.Subset(train_dataset, range(pieces[i], pieces[i+1])) for i in range(honestSize)]\n",
    "    train_loaders_splited = [\n",
    "        torch.utils.data.DataLoader(dataset=subset, batch_size=batchSize, shuffle=False)\n",
    "        for subset in train_dataset_subset\n",
    "    ]\n",
    "    \n",
    "    # 随机取样器\n",
    "    randomSampler = lambda dataset: torch.utils.data.sampler.RandomSampler( \n",
    "        dataset, \n",
    "        num_samples=rounds*displayInterval*batchSize, \n",
    "        replacement=True #有放回取样\n",
    "    )\n",
    "    train_random_loaders_splited = [torch.utils.data.DataLoader(\n",
    "        dataset=subset,\n",
    "        batch_size=batchSize, \n",
    "        sampler=randomSampler(subset),\n",
    "    ) for subset in train_dataset_subset]\n",
    "    \n",
    "    randomIters = [iter(loader) for loader in train_random_loaders_splited]\n",
    "    \n",
    "    # 求初始误差\n",
    "    trainLoss, trainAccuracy = calculateAccuracy(model, train_loader, device)\n",
    "    valLoss, valAccuracy = calculateAccuracy(model, validate_loader, device)\n",
    "    \n",
    "    trainLossPath = [trainLoss]\n",
    "    trainAccPath = [trainAccuracy]\n",
    "    valLossPath = [valLoss]\n",
    "    valAccPath = [valAccuracy]\n",
    "    variencePath = []\n",
    "    \n",
    "    report(0, rounds, displayInterval, trainLoss, trainAccuracy, valLoss, valAccuracy)\n",
    "\n",
    "    for r in range(rounds):\n",
    "        for k in range(displayInterval):\n",
    "            # 诚实节点更新\n",
    "            for node in range(honestSize):\n",
    "                # 读取数据\n",
    "                material, targets = next(randomIters[node]) \n",
    "                \n",
    "                # 随机梯度\n",
    "                # --------------------\n",
    "                # 预测\n",
    "                outputs = model(material)\n",
    "                loss = loss_func(outputs, targets)\n",
    "                # 反向传播\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # 更新梯度表\n",
    "                for pi, para in enumerate(model.parameters()):\n",
    "                    message[node][pi].data.zero_()\n",
    "                    message[node][pi].data.add_(1, para.grad.data)\n",
    "                    message[node][pi].data.add_(weight_decay, para)\n",
    "\n",
    "            # 同步, Byzantine攻击\n",
    "            message_f = flatten_list(message, byzantineSize) \n",
    "            if attack != None:\n",
    "                attack(message_f, byzantineSize)\n",
    "            # 聚合\n",
    "            g_vector = aggregate(message_f)\n",
    "            # 展开\n",
    "            g = unflatten_vector(g_vector, model) \n",
    "            # 更新\n",
    "            for para, grad in zip(model.parameters(), g):\n",
    "                para.data.add_(-gamma, grad)\n",
    "  \n",
    "        var = getVarience(message_f, honestSize)\n",
    "        variencePath.append(var)\n",
    "        \n",
    "        trainLoss, trainAccuracy = calculateAccuracy(model, train_loader, device)\n",
    "        valLoss, valAccuracy = calculateAccuracy(model, validate_loader, device)\n",
    "\n",
    "        trainLossPath.append(trainLoss)\n",
    "        trainAccPath.append(trainAccuracy)\n",
    "        valLossPath.append(valLoss)\n",
    "        valAccPath.append(valAccuracy)\n",
    "        \n",
    "        report(r+1, rounds, displayInterval, trainLoss, trainAccuracy, valLoss, valAccuracy)\n",
    "    return model, trainLossPath, trainAccPath, valLossPath, valAccPath, variencePath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## SAGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 初始化本地模型\n",
    "def initModel(local_models, honestSize):\n",
    "    stateDict = local_models[0].state_dict()\n",
    "    for model in local_models[1:honestSize]:\n",
    "        model.load_state_dict(stateDict)\n",
    "\n",
    "# 广播\n",
    "def broadcastPara(newPara, local_models):\n",
    "    cum = 0\n",
    "    for p in local_models[0].parameters():\n",
    "        newP = newPara[cum:cum+p.numel()]\n",
    "        p.data.copy_(newP.view_as(p))\n",
    "        cum += p.numel()\n",
    "    stateDict = local_models[0].state_dict()\n",
    "    for model in local_models[1:]:\n",
    "        model.load_state_dict(stateDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     37
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def SAGA(model, gamma, aggregate, weight_decay, \n",
    "          honestSize=0, byzantineSize=0, attack=None, \n",
    "          rounds=10, displayInterval=1000, \n",
    "          device='cpu', SEED=100, fixSeed=False, \n",
    "          batchSize=1,\n",
    "          **kw):\n",
    "    assert byzantineSize == 0 or attack != None\n",
    "    assert honestSize != 0\n",
    "    \n",
    "    if fixSeed:\n",
    "        random.seed(SEED)\n",
    "\n",
    "    nodeSize = honestSize + byzantineSize\n",
    "    \n",
    "    # 数据分片\n",
    "    pieces = [(i*len(train_dataset)) // honestSize for i in range(honestSize+1)]\n",
    "    dataPerNode = [pieces[i+1] - pieces[i] for i in range(honestSize)]\n",
    "    \n",
    "    #创建变量\n",
    "    store = []\n",
    "    \n",
    "    # 顺序遍历loader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batchSize, shuffle=False)\n",
    "    validate_loader = torch.utils.data.DataLoader(dataset=validate_dataset, batch_size=batchSize, shuffle=False)\n",
    "    \n",
    "    train_dataset_subset = [torch.utils.data.Subset(train_dataset, range(pieces[i], pieces[i+1])) for i in range(honestSize)]\n",
    "    train_loaders_splited = [\n",
    "        torch.utils.data.DataLoader(dataset=subset, batch_size=batchSize, shuffle=False)\n",
    "        for subset in train_dataset_subset\n",
    "    ]\n",
    "    \n",
    "    # 随机取样器\n",
    "    randomSampler = lambda dataset: torch.utils.data.sampler.RandomSampler( \n",
    "        dataset, \n",
    "        num_samples=rounds*displayInterval*batchSize, #取样规模：10*1500*batchSize \n",
    "        replacement=True #有放回取样\n",
    "    )\n",
    "    train_random_loaders_splited = [torch.utils.data.DataLoader(\n",
    "        dataset=subset,\n",
    "        batch_size=batchSize, \n",
    "        sampler=randomSampler(subset),\n",
    "    ) for subset in train_dataset_subset]\n",
    "    \n",
    "    randomIters = [iter(loader) for loader in train_random_loaders_splited]\n",
    "    \n",
    "    # 求初始误差\n",
    "    trainLoss, trainAccuracy = calculateAccuracy(model, train_loader, device)\n",
    "    valLoss, valAccuracy = calculateAccuracy(model, validate_loader, device)\n",
    "    \n",
    "    trainLossPath = [trainLoss]\n",
    "    trainAccPath = [trainAccuracy]\n",
    "    valLossPath = [valLoss]\n",
    "    valAccPath = [valAccuracy]\n",
    "    variencePath = []\n",
    "    \n",
    "    report(0, rounds, displayInterval, trainLoss, trainAccuracy, valLoss, valAccuracy)\n",
    "    \n",
    "    #对所有样本的权重梯度进行初始化\n",
    "    for index, (material, targets) in enumerate(train_dataset):\n",
    "        # 计算Loss\n",
    "        outputs = model(material)\n",
    "        targets = torch.tensor([targets])        \n",
    "        loss = loss_func(outputs, targets)\n",
    "        \n",
    "        # 反向传播\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        store.append([p.grad.clone().detach() for p in model.parameters()])\n",
    "    \n",
    "    # G_avg每一行是单个节点上存储的均值\n",
    "    G_avg = []\n",
    "    for i in range(honestSize):\n",
    "        # storeInThisNode：该节点上梯度缓存的集合\n",
    "        storeInThisNode = store[pieces[i]: pieces[i+1]]\n",
    "        # para每一个元素是在对应节点上的一组参数\n",
    "        (*paras,) = zip(*storeInThisNode)\n",
    "        # 对所有单一节点上所有数据求平均\n",
    "        G_avg.append([sum(para)/(pieces[i+1]-pieces[i]) for para in paras])\n",
    "    \n",
    "    # 回复的消息\n",
    "    message = [\n",
    "        [torch.zeros_like(para, requires_grad=False) for para in model.parameters()]\n",
    "        for _ in range(nodeSize)\n",
    "    ]\n",
    "    \n",
    "    for r in range(rounds):\n",
    "        for k in range(displayInterval):\n",
    "            # 诚实节点更新\n",
    "            for node in range(honestSize):\n",
    "                # 读取数据\n",
    "                index = random.randint(pieces[node], pieces[node+1]-1)\n",
    "                # 预测\n",
    "                material, targets = train_dataset[index]\n",
    "                # 计算Loss\n",
    "                outputs = model(material)\n",
    "                targets = torch.tensor([targets])\n",
    "                loss = loss_func(outputs, targets)\n",
    "                \n",
    "                # 反向传播\n",
    "                model.zero_grad()                \n",
    "                loss.backward()\n",
    "\n",
    "                # 更新梯度表\n",
    "                for pi, para in enumerate(model.parameters()):\n",
    "                    old_G = store[index][pi]\n",
    "                    new_G = para.grad.data.clone()\n",
    "                    new_G.add_(weight_decay, para.data)\n",
    "\n",
    "                    gradient = new_G.data - old_G.data + G_avg[node][pi].data\n",
    "                    \n",
    "                    message[node][pi] =gradient\n",
    "\n",
    "                    G_avg[node][pi].add_(1 / dataPerNode[node],new_G.data - old_G.data)\n",
    "                    \n",
    "                    store[index][pi] = new_G.data\n",
    "                \n",
    "                #攻击\n",
    "                message_f = flatten_list(message, byzantineSize) #将原本parameters的tensor形式压缩成torch.Size([90, 39760])\n",
    "                if attack != None:\n",
    "                    attack(message_f, byzantineSize)\n",
    "                # 聚合\n",
    "                g_vector = aggregate(message_f)\n",
    "                # 展开\n",
    "                g = unflatten_vector(g_vector, model) #展开成原本parameters的tensor形式\n",
    "                # 更新\n",
    "                for para, grad in zip(model.parameters(), g):\n",
    "                    para.data.add_(-gamma, grad)\n",
    "        \n",
    "        var = getVarience(message_f, honestSize)\n",
    "        variencePath.append(var)\n",
    "        \n",
    "        trainLoss, trainAccuracy = calculateAccuracy(model, train_loader, device)\n",
    "        valLoss, valAccuracy = calculateAccuracy(model, validate_loader, device)\n",
    "\n",
    "        trainLossPath.append(trainLoss)\n",
    "        trainAccPath.append(trainAccuracy)\n",
    "        valLossPath.append(valLoss)\n",
    "        valAccPath.append(valAccuracy)\n",
    "        \n",
    "        report(r+1, rounds, displayInterval, trainLoss, trainAccuracy, valLoss, valAccuracy)\n",
    "    return model, trainLossPath, trainAccPath, valLossPath, valAccPath, variencePath              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## SVRG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     37
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def SVRG(w0, gamma, aggregate, weight_decay, honestSize=0, byzantineSize=0, attack=None, \n",
    "            snapshotInterval=6000, rounds=10, displayInterval=1000, SEED=100, fixSeed=False, **kw):\n",
    "    assert byzantineSize == 0 or attack != None\n",
    "    assert honestSize != 0\n",
    "    \n",
    "    if fixSeed:\n",
    "        random.seed(SEED)\n",
    "\n",
    "    nodeSize = honestSize + byzantineSize\n",
    "    \n",
    "    # 初始化\n",
    "    w = w0.clone().detach()\n",
    "\n",
    "    # 数据分片\n",
    "    pieces = [(i*len(dataset)) // honestSize for i in range(honestSize+1)]\n",
    "    dataPerNode = [pieces[i+1] - pieces[i] for i in range(honestSize)]\n",
    "\n",
    "    snapshot_g = torch.zeros(honestSize, len(w0), dtype=torch.float64)\n",
    "    snapshot_w = torch.zeros(len(w0), dtype=torch.float64)\n",
    "\n",
    "    path = [F(w, dataset, weight_decay)]\n",
    "    variencePath = []\n",
    "    log('[SVRG]初始 loss={:.6f}, accuracy={:.2f} gamma={:}'.format(path[0], accuracy(w, dataset), gamma))\n",
    "    \n",
    "    # 中间变量分配空间\n",
    "    message = torch.zeros(nodeSize, len(w0), dtype=torch.float64)\n",
    "\n",
    "    log('开始迭代')\n",
    "    for r in range(rounds):\n",
    "        for k in range(displayInterval):\n",
    "            # snapshot\n",
    "            if (r*displayInterval + k) % snapshotInterval == 0:\n",
    "                snapshot_g.zero_()\n",
    "                for node in range(honestSize):\n",
    "                    for index in range(pieces[node], pieces[node+1]):\n",
    "                        x, y = dataset[index]\n",
    "                        # 更新梯度表\n",
    "                        predict = LogisticRegression(w, x)\n",
    "\n",
    "                        err = (predict-y).data\n",
    "                        snapshot_g[node][:-1].add_(1/dataPerNode[node], err*x)\n",
    "                        snapshot_g[node][-1].add_(1/dataPerNode[node], err)\n",
    "                    snapshot_g[node].add_(weight_decay, w)\n",
    "                snapshot_w.copy_(w)\n",
    "            \n",
    "            # 诚实节点更新\n",
    "            message.zero_()\n",
    "            for node in range(honestSize):\n",
    "                index = random.randint(pieces[node], pieces[node+1]-1)\n",
    "\n",
    "                x, y = dataset[index]\n",
    "                # 随机梯度\n",
    "                predict = LogisticRegression(w, x)\n",
    "                err = (predict-y).data\n",
    "                message[node][:-1].add_(err, x)\n",
    "                message[node][-1].add_(err, 1)\n",
    "                message[node].add_(weight_decay, w)\n",
    "                \n",
    "                # 修正梯度\n",
    "                predict = LogisticRegression(snapshot_w, x)\n",
    "                err = (predict-y).data\n",
    "                message[node][:-1].add_(-err, x)\n",
    "                message[node][-1].add_(-err, 1)\n",
    "                message[node].add_(-weight_decay, snapshot_w)\n",
    "                \n",
    "                message[node].add_(1, snapshot_g[node])\n",
    "                \n",
    "            # 同步\n",
    "            # Byzantine攻击\n",
    "            if attack != None:\n",
    "                attack(message, byzantineSize)\n",
    "            g = aggregate(message)\n",
    "            w.add_(-gamma, g)\n",
    "            \n",
    "        loss = F(w, dataset, weight_decay)\n",
    "        acc = accuracy(w, dataset)\n",
    "        path.append(loss)\n",
    "        var = getVarience(message, honestSize)\n",
    "        variencePath.append(var)\n",
    "        log('[SVRG]已迭代 {}/{} rounds (interval: {:.0f}), loss={:.9f}, accuracy={:.2f}, var={:.9f}'.format(\n",
    "            r+1, rounds, displayInterval, loss, acc, var\n",
    "        ))\n",
    "    return w, path, variencePath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## SARAH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def SARAH(model, gamma, aggregate, weight_decay, \n",
    "          snapshotInterval=len(train_dataset),\n",
    "          honestSize=0, byzantineSize=0, attack=None, \n",
    "          rounds=10, displayInterval=1000, \n",
    "          device='cpu', SEED=100, fixSeed=False, \n",
    "          batchSize=5,\n",
    "          **kw):\n",
    "    assert byzantineSize == 0 or attack != None\n",
    "    assert honestSize != 0\n",
    "    \n",
    "    if fixSeed:\n",
    "        random.seed(SEED)\n",
    "\n",
    "    nodeSize = honestSize + byzantineSize\n",
    "    \n",
    "    # 初始化模型\n",
    "    lastModel = modelFactory(SEED=SEED)\n",
    "\n",
    "    if device == 'cpu':\n",
    "        torch.manual_seed(SEED)#为CPU设置随机种子\n",
    "    else:\n",
    "        torch.cuda.manual_seed(seed)#为当前GPU设置随机种子\n",
    "        torch.cuda.manual_seed_all(seed)#为所有GPU设置随机种子\n",
    "    \n",
    "    # 数据分片\n",
    "    pieces = [(i*len(train_dataset)) // honestSize for i in range(honestSize+1)]\n",
    "    dataPerNode = [pieces[i+1] - pieces[i] for i in range(honestSize)]\n",
    "\n",
    "    # 随机的停止期限\n",
    "    randomStop = 1\n",
    "    # 回复的消息\n",
    "    message = [\n",
    "        [torch.zeros_like(para, requires_grad=False) for para in model.parameters()]\n",
    "        for _ in range(nodeSize)\n",
    "    ]\n",
    "    \n",
    "    # 顺序遍历loader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batchSize, shuffle=False)\n",
    "    validate_loader = torch.utils.data.DataLoader(dataset=validate_dataset, batch_size=batchSize, shuffle=False)\n",
    "    \n",
    "    train_dataset_subset = [torch.utils.data.Subset(train_dataset, range(pieces[i], pieces[i+1])) for i in range(honestSize)]\n",
    "    train_loaders_splited = [\n",
    "        torch.utils.data.DataLoader(dataset=subset, batch_size=batchSize, shuffle=False)\n",
    "        for subset in train_dataset_subset\n",
    "    ]\n",
    "    \n",
    "    # 随机取样器\n",
    "    randomSampler = lambda dataset: torch.utils.data.sampler.RandomSampler(\n",
    "        dataset, \n",
    "        num_samples=rounds*displayInterval*batchSize, \n",
    "        replacement=True\n",
    "    )\n",
    "    train_random_loaders_splited = [torch.utils.data.DataLoader(\n",
    "        dataset=subset,\n",
    "        batch_size=batchSize, \n",
    "        sampler=randomSampler(subset),\n",
    "    ) for subset in train_dataset_subset]\n",
    "    randomIters = [iter(loader) for loader in train_random_loaders_splited]\n",
    "    \n",
    "    # 求初始误差\n",
    "    trainLoss, trainAccuracy = calculateAccuracy(model, train_loader)\n",
    "    valLoss, valAccuracy = calculateAccuracy(model, validate_loader)\n",
    "    \n",
    "    trainLossPath = [trainLoss]\n",
    "    trainAccPath = [trainAccuracy]\n",
    "    valLossPath = [valLoss]\n",
    "    valAccPath = [valAccuracy]\n",
    "    variencePath = []\n",
    "    \n",
    "    log('[SARAH]初始 train: loss={:.6f} accuracy={:.2f} validation: loss={:.6f} accuracy={:.2f}'\n",
    "        .format(trainLossPath[0], trainAccPath[0], valLossPath[0], valAccPath[0])\n",
    "    )\n",
    "\n",
    "    for r in range(rounds):\n",
    "        for k in range(displayInterval):\n",
    "            # snapshot\n",
    "            if (r*displayInterval + k) % randomStop == 0:\n",
    "                for node in range(honestSize):\n",
    "                    # 清空旧梯度\n",
    "                    for grad in message[node]:\n",
    "                        grad.zero_()\n",
    "                    loader = train_loaders_splited[node]\n",
    "                    for material, targets in loader:\n",
    "                        # 预测\n",
    "                        outputs = model(material)\n",
    "                        loss = loss_func(outputs, targets)\n",
    "                        # 反向传播\n",
    "                        model.zero_grad()\n",
    "                        loss.backward()\n",
    "                        \n",
    "                        for grad, para in zip(message[node], model.parameters()):\n",
    "                            grad.data.add_(1/len(loader), para.grad.data)\n",
    "                    for grad, para in zip(message[node], model.parameters()):\n",
    "                        grad.data.add_(weight_decay, para.data)\n",
    "                \n",
    "                # 保存旧结果\n",
    "                for oldPara, newPara in zip(lastModel.parameters(), model.parameters()):\n",
    "                    oldPara.data.copy_(newPara)\n",
    "                # 同步, Byzantine攻击\n",
    "                message_f = flatten_list(message, byzantineSize)\n",
    "                if attack != None:\n",
    "                    attack(message_f, byzantineSize)\n",
    "                # 聚合\n",
    "                g_vector = aggregate(message_f)\n",
    "                # 展开\n",
    "                g = unflatten_vector(g_vector, model)\n",
    "                # 更新\n",
    "                for para, grad in zip(model.parameters(), g):\n",
    "                    para.data.add_(-gamma, grad)\n",
    "                # 指定下一次停止时间\n",
    "                randomStop = random.randint(1, snapshotInterval-1)\n",
    "                \n",
    "            # 诚实节点更新\n",
    "            for node in range(honestSize):\n",
    "                # 读取数据\n",
    "                material, targets = next(randomIters[node])\n",
    "                \n",
    "                # 随机梯度\n",
    "                # --------------------\n",
    "                # 预测\n",
    "                outputs = model(material)\n",
    "                loss = loss_func(outputs, targets)\n",
    "                # 反向传播\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                \n",
    "                # 修正梯度\n",
    "                # --------------------\n",
    "                # 预测\n",
    "                outputs = lastModel(material)\n",
    "                loss = loss_func(outputs, targets)\n",
    "                # 反向传播\n",
    "                lastModel.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # 更新梯度表\n",
    "                for pi, para in enumerate(model.parameters()):\n",
    "                    message[node][pi].data.add_(1, para.grad.data)\n",
    "                    message[node][pi].data.add_(weight_decay, para)\n",
    "                for pi, para in enumerate(lastModel.parameters()):\n",
    "                    message[node][pi].data.sub_(1, para.grad.data)\n",
    "                    message[node][pi].data.sub_(weight_decay, para)\n",
    "\n",
    "            # 同步, Byzantine攻击\n",
    "            message_f = flatten_list(message, byzantineSize)\n",
    "            if attack != None:\n",
    "                attack(message_f, byzantineSize)\n",
    "            # 聚合\n",
    "            g_vector = aggregate(message_f)\n",
    "            # 展开\n",
    "            g = unflatten_vector(g_vector, model)\n",
    "            # 保存旧结果\n",
    "            for oldPara, newPara in zip(lastModel.parameters(), model.parameters()):\n",
    "                oldPara.data.copy_(newPara)\n",
    "            # 更新\n",
    "            for para, grad in zip(model.parameters(), g):\n",
    "                para.data.add_(-gamma, grad)\n",
    "  \n",
    "        var = getVarience(message_f, honestSize)\n",
    "        variencePath.append(var)\n",
    "        \n",
    "        trainLoss, trainAccuracy = calculateAccuracy(model, train_loader)\n",
    "        valLoss, valAccuracy = calculateAccuracy(model, validate_loader)\n",
    "\n",
    "        trainLossPath.append(trainLoss)\n",
    "        trainAccPath.append(trainAccuracy)\n",
    "        valLossPath.append(valLoss)\n",
    "        valAccPath.append(valAccuracy)\n",
    "        \n",
    "        report(r+1, rounds, displayInterval, trainLoss, trainAccuracy, valLoss, valAccuracy)\n",
    "    return model, trainLossPath, trainAccPath, valLossPath, valAccPath, variencePath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 恶意攻击"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def white(messages, byzantinesize):\n",
    "    # 均值相同，方差为30\n",
    "    mu = torch.mean(messages[0:-byzantinesize], dim=0)\n",
    "    messages[-byzantinesize:].copy_(mu)\n",
    "    noise = torch.randn((byzantinesize, messages.size(1)), dtype=torch.float64)\n",
    "    messages[-byzantinesize:].add_(30, noise)\n",
    "    \n",
    "def maxValue(messages, byzantinesize):\n",
    "    mu = torch.mean(messages[0:-byzantinesize], dim=0)\n",
    "    meliciousMessage = -10*mu\n",
    "    messages[-byzantinesize:].copy_(meliciousMessage)\n",
    "    \n",
    "def zeroGradient(messages, byzantinesize):\n",
    "    s = torch.sum(messages[0:-byzantinesize], dim=0)\n",
    "    messages[-byzantinesize:].copy_(-s / byzantinesize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_func, optimizer, trainloader, device, weight_decay):\n",
    "    \"\"\"\n",
    "    train model using loss_fn and optimizer in an epoch.\n",
    "    model: CNN networks\n",
    "    train_loader: a Dataloader object with training data\n",
    "    loss_func: loss function\n",
    "    device: train on cpu or gpu device\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    trainAccuracy = 0\n",
    "    trainLoss = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, (*material, targets) in enumerate(trainloader):\n",
    "        if isinstance(material, torch.Tensor):\n",
    "            material = material.to(device)\n",
    "        else:\n",
    "            material = [m.to(device) for m in material]\n",
    "        \n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(*material)\n",
    "        \n",
    "        loss = loss_func(outputs, targets)\n",
    "        trainLoss += loss.item()\n",
    "\n",
    "        # backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # AdamW - https://zhuanlan.zhihu.com/p/38945390\n",
    "        for group in optimizer.param_groups:\n",
    "            for param in group['params']:\n",
    "                param.data = param.data.add(-weight_decay * group['lr'], param.data)\n",
    "\n",
    "        # return the maximum value of each row of the input tensor in the \n",
    "        # given dimension dim, the second return vale is the index location\n",
    "        # of each maxium value found(argmax)\n",
    "        _, predicted = torch.max(outputs.data, dim=1)\n",
    "        trainAccuracy += (predicted == targets).sum().item()\n",
    "        \n",
    "        total += len(targets)\n",
    "    trainAccuracy /= total\n",
    "    trainLoss /= total\n",
    "    return trainLoss, trainAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loss_func, validateloader, device):\n",
    "    # evaluate the model\n",
    "    model.eval()\n",
    "    # context-manager that disabled gradient computation\n",
    "    with torch.no_grad():\n",
    "        # =============================================================\n",
    "        valAccuracy = 0\n",
    "        valLoss = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, (*material, targets) in enumerate(trainloader):\n",
    "            if isinstance(material, torch.Tensor):\n",
    "                material = material.to(device)\n",
    "            else:\n",
    "                material = [m.to(device) for m in material]\n",
    "\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(*material)\n",
    "            \n",
    "            loss = loss_func(outputs, targets)\n",
    "            valLoss += loss.item()\n",
    "            \n",
    "            # return the maximum value of each row of the input tensor in the \n",
    "            # given dimension dim, the second return vale is the index location\n",
    "            # of each maxium value found(argmax)\n",
    "            _, predicted = torch.max(outputs.data, dim=1)\n",
    "            valAccuracy += (predicted == targets).sum().item()\n",
    "            \n",
    "            total += len(targets)\n",
    "        valAccuracy /= total\n",
    "        valLoss /= total\n",
    "    return valLoss, valAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, testloader, classname=None, name='default'):\n",
    "    # evaluate the model\n",
    "    model.eval()\n",
    "    # context-manager that disabled gradient computation\n",
    "    with torch.no_grad():\n",
    "        result = []\n",
    "        test_cnt = 0\n",
    "        for i, (*material, targets) in enumerate(testloader):\n",
    "            if isinstance(material, torch.Tensor):\n",
    "                material = material.to(device)\n",
    "            else:\n",
    "                material = [m.to(device) for m in material]\n",
    "\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(*material)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, dim=1)\n",
    "\n",
    "            result.extend(predicted)\n",
    "            test_cnt += len(targets)\n",
    "\n",
    "    if classname != None:\n",
    "        result = [classname[i] for i in result]\n",
    "\n",
    "    log('共预测{}个数据'.format(test_cnt))\n",
    "    df_predict = pd.DataFrame({'id': list(range(1, len(result)+1)), 'polarity': result})\n",
    "    df_predict.to_csv('{}.csv'.format(name), index=False)\n",
    "    log('预测完成')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showCurve(list_trainLoss, list_trainAccuracy, list_valLoss, list_valAccuracy):\n",
    "    xAxis = list(range(len(list_trainLoss)))\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "    axs[0].plot(xAxis, list_trainLoss, label='train')\n",
    "    axs[0].plot(xAxis, list_valLoss, label='validation')\n",
    "    axs[0].set_title('Loss')\n",
    "\n",
    "    axs[1].plot(xAxis, list_trainAccuracy, label='train')\n",
    "    axs[1].plot(xAxis, list_valAccuracy, label='validation')\n",
    "    axs[1].set_title('Accuracy')\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.axis()\n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.set_ylabel('{}'.format(ax.get_title()))\n",
    "        ax.legend()\n",
    "    fig.set_size_inches((8, 4))\n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 运行函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(optimizer, aggregate, attack, config, device='cpu'):\n",
    "    # 初始化参数\n",
    "    _config = config.copy()\n",
    "    _config['aggregate'] = aggregate\n",
    "    _config['attack'] = attack\n",
    "    if attack == None:\n",
    "        _config['byzantineSize'] = 0\n",
    "        \n",
    "    model = modelFactory(SEED=_config['SEED'])\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 记录参数\n",
    "    attackName = 'baseline' if attack == None else attack.__name__\n",
    "    # e.g. Resnet50_SARAH(5)_baseline_mean\n",
    "    title = '{}_{}({})_{}_{}'.format(\n",
    "        model.__class__.__name__, \n",
    "        optimizer.__name__, \n",
    "        _config['batchSize'],\n",
    "        attackName, \n",
    "        aggregate.__name__\n",
    "    )\n",
    "    \n",
    "    # 打印运行信息\n",
    "    print('[提交任务] ' + title)\n",
    "    print('[运行信息]')\n",
    "    print('[网络属性]   name={} parameters number={}'.format(model.__class__.__name__, getPara(model)))\n",
    "    print('[优化方法]   name={} aggregation={} attack={}'.format(optimizer.__name__, aggregate.__name__, attackName))\n",
    "    print('[数据集属性] name={} trainSize={} validationSize={}'.format(dataSetConfig['name'], len(train_dataset), len(validate_dataset)))\n",
    "    print('[优化器设置] gamma={} weight_decay={} batchSize={}'.format(_config['gamma'], _config['weight_decay'], _config['batchSize']))\n",
    "    print('[节点个数]   honestSize={}, byzantineSize={}'.format(_config['honestSize'], _config['byzantineSize']))\n",
    "    print('[运行次数]   rounds={}, displayInterval={}'.format(_config['rounds'], _config['displayInterval']))\n",
    "    print('[torch设置]  device={}, SEED={}, fixSeed={}'.format(device, _config['SEED'], _config['fixSeed']))\n",
    "    print('-------------------------------------------')\n",
    "    \n",
    "    # 开始运行\n",
    "    log('优化开始')\n",
    "    res = optimizer(model, device=device, **_config)\n",
    "    [*model, trainLossPath, trainAccPath, valLossPath, valAccPath, variencePath] = res\n",
    "\n",
    "    record = {\n",
    "        **dataSetConfig,\n",
    "        **{key:(_config[key].__name__ if hasattr(_config[key], '__call__') else _config[key]) for key in _config},\n",
    "        'trainLossPath': trainLossPath, \n",
    "        'trainAccPath': trainAccPath, \n",
    "        'valLossPath': valLossPath, \n",
    "        'valAccPath': valAccPath, \n",
    "        'variencePath': variencePath,\n",
    "    }\n",
    "\n",
    "    with open(CACHE_DIR + title, 'wb') as f:\n",
    "        pickle.dump(record, f)\n",
    "    \n",
    "    _, axis = plt.subplots(1, 2)\n",
    "    axis[0].plot(list(range(len(trainLossPath))), trainLossPath, label='train loss')\n",
    "    axis[0].plot(list(range(len(valLossPath))), valLossPath, label='validation loss')\n",
    "    axis[1].plot(list(range(len(trainAccPath))), trainAccPath, label='train accuracy')\n",
    "    axis[1].plot(list(range(len(valAccPath))), valAccPath, label='validation accuracy')\n",
    "    for ax in axis:\n",
    "        ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中心式SGD调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_config = SGDConfig.copy()\n",
    "_config['gamma'] = 5e-1\n",
    "_config['rounds'] = 50\n",
    "_config['batchSize'] = 20\n",
    "run(optimizer = CentralSGD, aggregate = mean, attack = None, config = _config, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中心式SARAH调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_config = SARAHConfig.copy()\n",
    "_config['batchSize'] = 20\n",
    "_config['gamma'] = 1e-4\n",
    "_config['displayInterval'] = 100000\n",
    "_config['rounds'] = 30\n",
    "run(optimizer = CentralSARAH, aggregate = mean, attack = None, config = _config, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD - mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = mean, attack = None, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = mean, attack = white, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = mean, attack = maxValue, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = mean, attack = zeroGradient, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD - geomtric median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = gm, attack = zeroGradient, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = gm, attack = white, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = gm, attack = maxValue, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = gm, attack = zeroGradient, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD - Krum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Krum = Krum_(nodeSize=dataSetConfig['honestNodeSize'], byzantineSize=0)\n",
    "run(optimizer = SGD, aggregate = Krum, attack = None, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Krum = Krum_(nodeSize=dataSetConfig['honestNodeSize'], byzantineSize=0)\n",
    "run(optimizer = SGD, aggregate = Krum, attack = white, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Krum = Krum_(nodeSize=dataSetConfig['honestNodeSize'], byzantineSize=0)\n",
    "run(optimizer = SGD, aggregate = Krum, attack = maxValue, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Krum = Krum_(nodeSize=dataSetConfig['honestNodeSize'], byzantineSize=0)\n",
    "run(optimizer = SGD, aggregate = Krum, attack = zeroGradient, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD - Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = median, attack = None, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = median, attack = white, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = median, attack = maxValue, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = median, attack = zeroGradient, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchSGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchSGD - mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = BatchSGD, aggregate = mean, attack = None, config = batchConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = BatchSGD, aggregate = mean, attack = white, config = batchConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = BatchSGD, aggregate = mean, attack = maxValue, config = batchConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run(optimizer = BatchSGD, aggregate = mean, attack = zeroGradient, config = batchConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchSGD - geomtric median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = BatchSGD, aggregate = gm, attack = None, config = batchConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = BatchSGD, aggregate = gm, attack = white, config = batchConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = BatchSGD, aggregate = gm, attack = maxValue, config = batchConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = BatchSGD, aggregate = gm, attack = zeroGradient, config = batchConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAGA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAGA - mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = mean, attack = None, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = mean, attack = white, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = mean, attack = maxValue, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = mean, attack = zeroGradient, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAGA - geomtric median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = gm, attack = None, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = gm, attack = white, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = gm, attack = maxValue, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = gm, attack = zeroGradient, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAGA - Krum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Krum = Krum_(nodeSize=dataSetConfig['honestNodeSize'], byzantineSize=0)\n",
    "run(optimizer = SAGA, aggregate = Krum, attack = None, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Krum = Krum_(nodeSize=dataSetConfig['honestNodeSize'], byzantineSize=0)\n",
    "run(optimizer = SAGA, aggregate = Krum, attack = white, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Krum = Krum_(nodeSize=dataSetConfig['honestNodeSize'], byzantineSize=0)\n",
    "run(optimizer = SAGA, aggregate = Krum, attack = maxValue, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Krum = Krum_(nodeSize=dataSetConfig['honestNodeSize'], byzantineSize=0)\n",
    "run(optimizer = SAGA, aggregate = Krum, attack = zeroGradient, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAGA - Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = median, attack = None, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = median, attack = white, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = median, attack = maxValue, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = median, attack = zeroGradient, config = SAGAConfig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "713px",
    "left": "1485px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 基本定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 数据集属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "VRConfig = {\n",
    "    'rounds': 10,\n",
    "    'displayInterval': 4000,\n",
    "    \n",
    "    'weight_decay': 0.01,\n",
    "    'honestSize': 50,\n",
    "    'byzantineSize': 20,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataSetConfig = {\n",
    "    'name': 'ijcnn1',\n",
    "    'dataSet' : 'ijcnn1',\n",
    "    'dataSetSize': 49990,\n",
    "    'maxFeature': 22,\n",
    "    'findingType': '1',\n",
    "}\n",
    "\n",
    "VRConfig['SET_SIZE'] = dataSetConfig['dataSetSize']\n",
    "\n",
    "# ijcnn1参数\n",
    "SGDConfig = VRConfig.copy()\n",
    "SGDConfig['gamma'] = 2e-2\n",
    "\n",
    "batchConfig = VRConfig.copy()\n",
    "batchConfig['batchSize'] = 50\n",
    "batchConfig['gamma'] = 1e-2\n",
    "\n",
    "SVRGConfig = VRConfig.copy()\n",
    "SVRGConfig['snapshotInterval'] = dataSetConfig['dataSetSize']\n",
    "SVRGConfig['gamma'] = 2e-2\n",
    "\n",
    "SAGAConfig = VRConfig.copy()\n",
    "SAGAConfig['gamma'] = 2e-2\n",
    "\n",
    "SARAHConfig = VRConfig.copy()\n",
    "SARAHConfig['gamma'] = 2e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# covtype参数\n",
    "# dataSetConfig = {\n",
    "#     'name': 'covtype',\n",
    "#     'dataSet' : 'covtype.libsvm.binary.scale',\n",
    "#     'dataSetSize': 581012,\n",
    "#     'maxFeature': 54,\n",
    "#     'findingType': '1',\n",
    "# }\n",
    "\n",
    "# VRConfig['SET_SIZE'] = dataSetConfig['dataSetSize']\n",
    "\n",
    "# SGDConfig = VRConfig.copy()\n",
    "# SGDConfig['gamma'] = 1e-2\n",
    "\n",
    "# batchConfig = VRConfig.copy()\n",
    "# batchConfig['batchSize'] = 50\n",
    "# batchConfig['gamma'] = 5e-3\n",
    "\n",
    "# SVRGConfig = VRConfig.copy()\n",
    "# SVRGConfig['snapshotInterval'] = dataSetConfig['dataSetSize']\n",
    "# SVRGConfig['gamma'] = 1e-2\n",
    "\n",
    "# SAGAConfig = VRConfig.copy()\n",
    "# SAGAConfig['gamma'] = 1e-2\n",
    "\n",
    "# SARAHConfig = VRConfig.copy()\n",
    "# SARAHConfig['gamma'] = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SET_SIZE = dataSetConfig['dataSetSize']\n",
    "maxFeature = dataSetConfig['maxFeature']\n",
    "findingType = dataSetConfig['findingType']\n",
    "\n",
    "CACHE_DIR = './cache/' + dataSetConfig['name'] + '_'\n",
    "# ====================================================\n",
    "# 报告函数\n",
    "def log(*k, **kw):\n",
    "    timeStamp = time.strftime('[%y-%m-%d %H:%M:%S] ', time.localtime())\n",
    "    print(timeStamp, end='')\n",
    "    print(*k, **kw)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def logAxis(path, Fmin):\n",
    "#     return [math.log10(p-Fmin) for p in path]\n",
    "    return [p-Fmin for p in path]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 运行参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# L = np.sum([(scipy.sparse.linalg.norm(X[i, :]) + 1)\n",
    "#             ** 2 for i in range(X.shape[0])])\n",
    "# L = Lambda + 1/(4*SET_SIZE) * L\n",
    "\n",
    "SEED = 200\n",
    "\n",
    "torch.manual_seed(SEED)#为CPU设置随机种子\n",
    "\n",
    "w0 = torch.zeros(maxFeature + 1, dtype=torch.float64)\n",
    "w0 = torch.nn.init.normal_(w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SVM_dataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, **dataSetConfig):\n",
    "        super(SVM_dataSet, self).__init__()\n",
    "        log('开始加载数据集')\n",
    "        self.X = torch.zeros((SET_SIZE, maxFeature), dtype=torch.float64)\n",
    "        self.Y = torch.zeros((SET_SIZE), dtype=torch.float64)\n",
    "        __dir__ = '.'\n",
    "        dataFile = __dir__ + '/dataset/' + dataSetConfig['dataSet']\n",
    "\n",
    "        with open(dataFile, 'r') as f:\n",
    "            posCount = 0\n",
    "            negCount = 1\n",
    "            for (line, vector) in enumerate(f):\n",
    "                (cat, data) = vector.split(' ', 1)\n",
    "                if cat == findingType:\n",
    "                    self.Y[line] = 1\n",
    "                    posCount += 1\n",
    "                else:\n",
    "                    self.Y[line] = 0\n",
    "                    negCount += 1\n",
    "                for piece in data.strip().split(' '):\n",
    "                    match = re.search(r'(\\S+):(\\S+)', piece)\n",
    "                    feature = int(match.group(1)) - 1  # 数据集从1开始\n",
    "                    value = float(match.group(2))\n",
    "                    # 插入矩阵\n",
    "                    self.X[line][feature] = value\n",
    "        log('加载数据集完成({})，正类：{}个，负类：{}个'.format(dataSetConfig['dataSet'], posCount, negCount))\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.Y[index]\n",
    "    def __len__(self):\n",
    "        return SET_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset = SVM_dataSet(**dataSetConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def accuracy(w, dataset):\n",
    "    correct = 0\n",
    "    for data, label in dataset:\n",
    "        pre = LogisticRegression(w, data) > 0.5\n",
    "        correct += (pre.type(torch.uint8) == label.type(torch.uint8)).item()\n",
    "    return correct / len(dataset)\n",
    "def F(w, dataset, weight_decay):\n",
    "    loss = 0\n",
    "    for data, label in dataset:\n",
    "        predict = LogisticRegression(w, data)\n",
    "        loss += torch.nn.functional.binary_cross_entropy(predict, label)\n",
    "    loss /= len(dataset)\n",
    "    loss += weight_decay * torch.norm(w)**2 / 2\n",
    "    return loss.item()\n",
    "def G(w, dataset, weight_decay):\n",
    "    G = torch.zeros_like(w, requires_grad=False, dtype=torch.float64)\n",
    "    g = torch.zeros_like(w, requires_grad=False, dtype=torch.float64)\n",
    "    for index in range(len(dataset)):\n",
    "        x, y = dataset[index]\n",
    "        predict = LogisticRegression(w, x)\n",
    "\n",
    "        err = -(y-predict).data\n",
    "        g[:-1] = err*x\n",
    "        g[-1] = err\n",
    "        G.add_(1/len(dataset), g)\n",
    "    G.add_(weight_decay, w)\n",
    "    return G\n",
    "def LogisticRegression(w, x):\n",
    "    out = w[:-1].dot(x) + w[-1]\n",
    "    return torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getVarience(w_local, honestSize):\n",
    "    avg = w_local[:honestSize].mean(dim=0)\n",
    "    s = 0\n",
    "    for w in w_local[:honestSize]:\n",
    "        s += (w - avg).norm()**2\n",
    "    s /= honestSize\n",
    "    return s.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 聚合函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def mean(wList):\n",
    "    return torch.mean(wList, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def gm(wList):\n",
    "    max_iter = 80\n",
    "    tol = 1e-5\n",
    "    guess = torch.mean(wList, dim=0)\n",
    "    for _ in range(max_iter):\n",
    "        dist_li = torch.norm(wList-guess, dim=1)\n",
    "        for i in range(len(dist_li)):\n",
    "            if dist_li[i] == 0:\n",
    "                dist_li[i] = 1\n",
    "        temp1 = torch.sum(torch.stack([w/d for w, d in zip(wList, dist_li)]), dim=0)\n",
    "        temp2 = torch.sum(1/dist_li)\n",
    "        guess_next = temp1 / temp2\n",
    "        guess_movement = torch.norm(guess - guess_next)\n",
    "        guess = guess_next\n",
    "        if guess_movement <= tol:\n",
    "            break\n",
    "    return guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Krum_(nodeSize, byzantineSize):\n",
    "    honestSize = nodeSize - byzantineSize\n",
    "    dist = torch.zeros(nodeSize, nodeSize, dtype=torch.float32)\n",
    "    def Krum(wList):\n",
    "        for i in range(nodeSize):\n",
    "            for j in range(i, nodeSize):\n",
    "                distance = wList[i].data - wList[j].data\n",
    "                distance = (distance*distance).sum()\n",
    "                distance = -distance # 两处都是取距离的最小值，需要改成负数\n",
    "                dist[i][j] = distance.data\n",
    "                dist[j][i] = distance.data\n",
    "        k = nodeSize - byzantineSize - 2 + 1 # 算上自己和自己的0.00\n",
    "        topv, _ = dist.topk(k=k, dim=1)\n",
    "        sumdist = topv.sum(dim=1)\n",
    "        resindex = sumdist.topk(1)[1].squeeze()\n",
    "        return wList[resindex]\n",
    "    return Krum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def median(wList):\n",
    "    return wList.median(dim=0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Central SAGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def CentralSAGA(w0, dataset, gamma, weight_decay, epoch=1, **kw):\n",
    "\n",
    "    # 初始化\n",
    "    w = w0.clone().detach()\n",
    "    \n",
    "    store = torch.zeros([SET_SIZE, w.size(0)], requires_grad=False, dtype=torch.float64)\n",
    "    for index in range(SET_SIZE):\n",
    "        x, y = dataset[index]\n",
    "        predict = LogisticRegression(w, x)\n",
    "\n",
    "        err = (predict-y).data\n",
    "        store[index][:-1] = err*x\n",
    "        store[index][-1] = err\n",
    "        store[index].add_(weight_decay, w)\n",
    "\n",
    "    G_avg = torch.mean(store, dim=0)\n",
    "    path = [F(w, dataset, weight_decay)]\n",
    "    \n",
    "    # 中间变量分配空间\n",
    "    new_G = torch.zeros(w0.size(), dtype=torch.float64)\n",
    "    for e in range(epoch):\n",
    "        for _ in range(SET_SIZE):\n",
    "            # 更新梯度表\n",
    "            index = random.randint(0, SET_SIZE-1)\n",
    "\n",
    "            x, y = dataset[index]\n",
    "            predict = LogisticRegression(w, x)\n",
    "            \n",
    "            # 计算梯度\n",
    "            old_G = store[index]\n",
    "            err = (predict-y).data\n",
    "            new_G[:-1] = err*x\n",
    "            new_G[-1] = err\n",
    "            new_G.add_(weight_decay, w)\n",
    "            \n",
    "            gradient = new_G.data - old_G.data + G_avg.data\n",
    "            \n",
    "            G_avg.add_(1 / SET_SIZE, new_G.data - old_G.data)\n",
    "            store[index] = new_G.data\n",
    "            w.data.add_(-gamma, gradient.data)\n",
    "        loss = F(w, dataset, weight_decay)\n",
    "        path.append(loss)\n",
    "        log('[SAGA]已迭代{:.0f}/{:.0f}趟, loss={}'.format(e+1, epoch, loss))\n",
    "    return w, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def SAGA_min(w0, dataset, gamma, weight_decay, epoch=1, **kw):\n",
    "\n",
    "    # 初始化\n",
    "    w = w0.clone().detach()\n",
    "    \n",
    "    store = torch.zeros([SET_SIZE, w.size(0)], requires_grad=False, dtype=torch.float64)\n",
    "    for index in range(SET_SIZE):\n",
    "        x, y = dataset[index]\n",
    "        predict = LogisticRegression(w, x)\n",
    "\n",
    "        err = -(y-predict).data\n",
    "        store[index][:-1] = err*x\n",
    "        store[index][-1] = err\n",
    "        store[index].add_(weight_decay, w)\n",
    "\n",
    "    G_avg = torch.mean(store, dim=0)\n",
    "    \n",
    "    # 中间变量分配空间\n",
    "    new_G = torch.zeros(w0.size(), dtype=torch.float64)\n",
    "    for e in range(epoch):\n",
    "        for _ in range(SET_SIZE):\n",
    "            # 更新梯度表\n",
    "            index = random.randint(0, SET_SIZE-1)\n",
    "\n",
    "            x, y = dataset[index]\n",
    "            predict = LogisticRegression(w, x)\n",
    "            \n",
    "            # 计算梯度\n",
    "            old_G = store[index]\n",
    "            err = -(y-predict).data\n",
    "            new_G[:-1] = err*x\n",
    "            new_G[-1] = err\n",
    "            new_G.add_(weight_decay, w)\n",
    "            \n",
    "            gradient = new_G.data - old_G.data + G_avg.data\n",
    "            \n",
    "            G_avg.add_(1 / SET_SIZE, new_G.data - old_G.data)\n",
    "            store[index] = new_G.data\n",
    "            w.data.add_(-gamma, gradient.data)\n",
    "        log('[SAGA]已迭代{:.0f}/{:.0f}趟'.format(e+1, epoch))\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     37
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def SGD(w0, gamma, aggregate, weight_decay, honestSize=0, byzantineSize=0, attack=None,\n",
    "            rounds=10, displayInterval=1000, SEED=100, fixSeed=False, **kw):\n",
    "    assert byzantineSize == 0 or attack != None\n",
    "    assert honestSize != 0\n",
    "    \n",
    "    if fixSeed:\n",
    "        random.seed(SEED)\n",
    "\n",
    "    nodeSize = honestSize + byzantineSize\n",
    "    \n",
    "    # 初始化\n",
    "    w = w0.clone().detach()\n",
    "\n",
    "    # 数据分片\n",
    "    pieces = [(i*len(dataset)) // honestSize for i in range(honestSize+1)]\n",
    "    dataPerNode = [pieces[i+1] - pieces[i] for i in range(honestSize)]\n",
    "\n",
    "    path = [F(w, dataset, weight_decay)]\n",
    "    variencePath = []\n",
    "    log('[SGD]初始 loss={:.6f}, accuracy={:.2f} gamma={:}'.format(path[0], accuracy(w, dataset), gamma))\n",
    "    \n",
    "    # 中间变量分配空间\n",
    "    new_G = torch.zeros_like(w0, dtype=torch.float64)\n",
    "    message = torch.zeros(nodeSize, len(w0), dtype=torch.float64)\n",
    "\n",
    "    log('开始迭代')\n",
    "    for r in range(rounds):\n",
    "        for k in range(displayInterval):\n",
    "            # 诚实节点更新\n",
    "            for node in range(honestSize):\n",
    "                index = random.randint(pieces[node], pieces[node+1]-1)\n",
    "\n",
    "                x, y = dataset[index]\n",
    "                # 更新梯度表\n",
    "                predict = LogisticRegression(w, x)\n",
    "                err = (predict-y).data\n",
    "                new_G[:-1] = err*x\n",
    "                new_G[-1] = err\n",
    "                new_G.add_(weight_decay, w)\n",
    "                \n",
    "                gradient = new_G\n",
    "                \n",
    "                message[node].copy_(gradient.data)\n",
    "\n",
    "            # 同步\n",
    "            # Byzantine攻击\n",
    "            if attack != None:\n",
    "                attack(message, byzantineSize)\n",
    "            g = aggregate(message)\n",
    "            w.add_(-gamma, g.data)\n",
    "            \n",
    "        loss = F(w, dataset, weight_decay)\n",
    "        acc = accuracy(w, dataset)\n",
    "        path.append(loss)\n",
    "        var = getVarience(message, honestSize)\n",
    "        variencePath.append(var)\n",
    "        log('[SGD]已迭代 {}/{} rounds (interval: {:.0f}), loss={:.9f}, accuracy={:.2f}, var={:.9f}'.format(\n",
    "            r+1, rounds, displayInterval, loss, acc, var\n",
    "        ))\n",
    "    return w, path, variencePath\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## BatchSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     37
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def BatchSGD(w0, gamma, aggregate, weight_decay, honestSize=0, byzantineSize=0, attack=None, batchSize=50,\n",
    "            rounds=10, displayInterval=1000, SEED=100, fixSeed=False, **kw):\n",
    "    assert byzantineSize == 0 or attack != None\n",
    "    assert honestSize != 0\n",
    "    \n",
    "    if fixSeed:\n",
    "        random.seed(SEED)\n",
    "\n",
    "    nodeSize = honestSize + byzantineSize\n",
    "    \n",
    "    # 初始化\n",
    "    w = w0.clone().detach()\n",
    "\n",
    "    # 数据分片\n",
    "    pieces = [(i*len(dataset)) // honestSize for i in range(honestSize+1)]\n",
    "    dataPerNode = [pieces[i+1] - pieces[i] for i in range(honestSize)]\n",
    "\n",
    "    path = [F(w, dataset, weight_decay)]\n",
    "    variencePath = []\n",
    "    log('[BatchSGD]初始 loss={:.6f}, accuracy={:.2f} gamma={:}'.format(path[0], accuracy(w, dataset), gamma))\n",
    "    \n",
    "    # 中间变量分配空间\n",
    "    new_G = torch.zeros_like(w0, dtype=torch.float64)\n",
    "    message = torch.zeros(nodeSize, len(w0), dtype=torch.float64)\n",
    "\n",
    "    log('开始迭代')\n",
    "    for r in range(rounds):\n",
    "        for k in range(displayInterval):\n",
    "            # 诚实节点更新\n",
    "            for node in range(honestSize):\n",
    "                gradient = torch.zeros_like(new_G)\n",
    "                for b in range(batchSize):\n",
    "                    index = random.randint(pieces[node], pieces[node+1]-1)\n",
    "\n",
    "                    x, y = dataset[index]\n",
    "                    # 更新梯度表\n",
    "                    predict = LogisticRegression(w, x)\n",
    "                    err = (predict-y).data\n",
    "                    new_G[:-1] = err*x\n",
    "                    new_G[-1] = err\n",
    "                    new_G.add_(weight_decay, w)\n",
    "                    gradient.add_(1/batchSize, new_G)\n",
    "                message[node].copy_(gradient.data)\n",
    "\n",
    "            # 同步\n",
    "            # Byzantine攻击\n",
    "            if attack != None:\n",
    "                attack(message, byzantineSize)\n",
    "            g = aggregate(message)\n",
    "            w.add_(-gamma, g.data)\n",
    "            \n",
    "        loss = F(w, dataset, weight_decay)\n",
    "        acc = accuracy(w, dataset)\n",
    "        path.append(loss)\n",
    "        var = getVarience(message, honestSize)\n",
    "        variencePath.append(var)\n",
    "        log('[BatchSGD]已迭代 {}/{} rounds (interval: {:.0f}), loss={:.9f}, accuracy={:.2f}, var={:.9f}'.format(\n",
    "            r+1, rounds, displayInterval, loss, acc, var\n",
    "        ))\n",
    "    return w, path, variencePath\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## SAGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     37
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def SAGA(w0, gamma, aggregate, weight_decay, honestSize=0, byzantineSize=0, attack=None, \n",
    "            rounds=10, displayInterval=1000, SEED=100, fixSeed=False, **kw):\n",
    "    assert byzantineSize == 0 or attack != None\n",
    "    assert honestSize != 0\n",
    "    \n",
    "    if fixSeed:\n",
    "        random.seed(SEED)\n",
    "\n",
    "    nodeSize = honestSize + byzantineSize\n",
    "    \n",
    "    # 初始化\n",
    "    w = w0.clone().detach()\n",
    "\n",
    "    store = torch.zeros([len(dataset), w.size(0)], requires_grad=False, dtype=torch.float64)\n",
    "    for index in range(len(dataset)):\n",
    "        x, y = dataset[index]\n",
    "        predict = LogisticRegression(w, x)\n",
    "\n",
    "        err = (predict-y).data\n",
    "        store[index][:-1] = err*x\n",
    "        store[index][-1] = err\n",
    "        store[index].add_(weight_decay, w)\n",
    "\n",
    "    # 数据分片\n",
    "    pieces = [(i*len(dataset)) // honestSize for i in range(honestSize+1)]\n",
    "    dataPerNode = [pieces[i+1] - pieces[i] for i in range(honestSize)]\n",
    "\n",
    "    G_avg = torch.stack([\n",
    "        store[pieces[i]:pieces[i+1]].mean(dim=0) for i in range(honestSize)\n",
    "    ])\n",
    "    path = [F(w, dataset, weight_decay)]\n",
    "    variencePath = []\n",
    "    log('[SAGA]初始 loss={:.6f}, accuracy={:.2f} gamma={:}'.format(path[0], accuracy(w, dataset), gamma))\n",
    "    \n",
    "    # 中间变量分配空间\n",
    "    new_G = torch.zeros_like(w0, dtype=torch.float64)\n",
    "    message = torch.zeros(nodeSize, len(w0), dtype=torch.float64)\n",
    "\n",
    "    log('开始迭代')\n",
    "    for r in range(rounds):\n",
    "        for k in range(displayInterval):\n",
    "            # 诚实节点更新\n",
    "            for node in range(honestSize):\n",
    "                index = random.randint(pieces[node], pieces[node+1]-1)\n",
    "\n",
    "                x, y = dataset[index]\n",
    "                # 更新梯度表\n",
    "                predict = LogisticRegression(w, x)\n",
    "\n",
    "                old_G = store[index]\n",
    "                err = (predict-y).data\n",
    "                new_G[:-1] = err*x\n",
    "                new_G[-1] = err\n",
    "                new_G.add_(weight_decay, w)\n",
    "\n",
    "                gradient = new_G.data - old_G.data + G_avg[node].data\n",
    "\n",
    "                G_avg[node].add_(1 / dataPerNode[node],\n",
    "                                 new_G.data - old_G.data)\n",
    "                store[index] = new_G.data\n",
    "\n",
    "                message[node].copy_(gradient.data)\n",
    "\n",
    "            # 同步\n",
    "            # Byzantine攻击\n",
    "            if attack != None:\n",
    "                attack(message, byzantineSize)\n",
    "            g = aggregate(message)\n",
    "            w.add_(-gamma, g.data)\n",
    "            \n",
    "        loss = F(w, dataset, weight_decay)\n",
    "        acc = accuracy(w, dataset)\n",
    "        path.append(loss)\n",
    "        var = getVarience(message, honestSize)\n",
    "        variencePath.append(var)\n",
    "        log('[SAGA]已迭代 {}/{} rounds (interval: {:.0f}), loss={:.9f}, accuracy={:.2f}, var={:.9f}'.format(\n",
    "            r+1, rounds, displayInterval, loss, acc, var\n",
    "        ))\n",
    "    return w, path, variencePath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## SVRG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     37
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def SVRG(w0, gamma, aggregate, weight_decay, honestSize=0, byzantineSize=0, attack=None, \n",
    "            snapshotInterval=6000, rounds=10, displayInterval=1000, SEED=100, fixSeed=False, **kw):\n",
    "    assert byzantineSize == 0 or attack != None\n",
    "    assert honestSize != 0\n",
    "    \n",
    "    if fixSeed:\n",
    "        random.seed(SEED)\n",
    "\n",
    "    nodeSize = honestSize + byzantineSize\n",
    "    \n",
    "    # 初始化\n",
    "    w = w0.clone().detach()\n",
    "\n",
    "    # 数据分片\n",
    "    pieces = [(i*len(dataset)) // honestSize for i in range(honestSize+1)]\n",
    "    dataPerNode = [pieces[i+1] - pieces[i] for i in range(honestSize)]\n",
    "\n",
    "    snapshot_g = torch.zeros(honestSize, len(w0), dtype=torch.float64)\n",
    "    snapshot_w = torch.zeros(len(w0), dtype=torch.float64)\n",
    "\n",
    "    path = [F(w, dataset, weight_decay)]\n",
    "    variencePath = []\n",
    "    log('[SVRG]初始 loss={:.6f}, accuracy={:.2f} gamma={:}'.format(path[0], accuracy(w, dataset), gamma))\n",
    "    \n",
    "    # 中间变量分配空间\n",
    "    message = torch.zeros(nodeSize, len(w0), dtype=torch.float64)\n",
    "\n",
    "    log('开始迭代')\n",
    "    for r in range(rounds):\n",
    "        for k in range(displayInterval):\n",
    "            # snapshot\n",
    "            if (r*displayInterval + k) % snapshotInterval == 0:\n",
    "                snapshot_g.zero_()\n",
    "                for node in range(honestSize):\n",
    "                    for index in range(pieces[node], pieces[node+1]):\n",
    "                        x, y = dataset[index]\n",
    "                        # 更新梯度表\n",
    "                        predict = LogisticRegression(w, x)\n",
    "\n",
    "                        err = (predict-y).data\n",
    "                        snapshot_g[node][:-1].add_(1/dataPerNode[node], err*x)\n",
    "                        snapshot_g[node][-1].add_(1/dataPerNode[node], err)\n",
    "                    snapshot_g[node].add_(weight_decay, w)\n",
    "                snapshot_w.copy_(w)\n",
    "            \n",
    "            # 诚实节点更新\n",
    "            message.zero_()\n",
    "            for node in range(honestSize):\n",
    "                index = random.randint(pieces[node], pieces[node+1]-1)\n",
    "\n",
    "                x, y = dataset[index]\n",
    "                # 随机梯度\n",
    "                predict = LogisticRegression(w, x)\n",
    "                err = (predict-y).data\n",
    "                message[node][:-1].add_(err, x)\n",
    "                message[node][-1].add_(err, 1)\n",
    "                message[node].add_(weight_decay, w)\n",
    "                \n",
    "                # 修正梯度\n",
    "                predict = LogisticRegression(snapshot_w, x)\n",
    "                err = (predict-y).data\n",
    "                message[node][:-1].add_(-err, x)\n",
    "                message[node][-1].add_(-err, 1)\n",
    "                message[node].add_(-weight_decay, snapshot_w)\n",
    "                \n",
    "                message[node].add_(1, snapshot_g[node])\n",
    "                \n",
    "            # 同步\n",
    "            # Byzantine攻击\n",
    "            if attack != None:\n",
    "                attack(message, byzantineSize)\n",
    "            g = aggregate(message)\n",
    "            w.add_(-gamma, g)\n",
    "            \n",
    "        loss = F(w, dataset, weight_decay)\n",
    "        acc = accuracy(w, dataset)\n",
    "        path.append(loss)\n",
    "        var = getVarience(message, honestSize)\n",
    "        variencePath.append(var)\n",
    "        log('[SVRG]已迭代 {}/{} rounds (interval: {:.0f}), loss={:.9f}, accuracy={:.2f}, var={:.9f}'.format(\n",
    "            r+1, rounds, displayInterval, loss, acc, var\n",
    "        ))\n",
    "    return w, path, variencePath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## SARAH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def SARAH(w0, gamma, aggregate, weight_decay, honestSize=0, byzantineSize=0, attack=None, \n",
    "            snapshotInterval=6000, rounds=10, displayInterval=1000, SEED=100, fixSeed=False, **kw):\n",
    "    assert byzantineSize == 0 or attack != None\n",
    "    assert honestSize != 0\n",
    "    \n",
    "    if fixSeed:\n",
    "        random.seed(SEED)\n",
    "\n",
    "    nodeSize = honestSize + byzantineSize\n",
    "    \n",
    "    # 初始化\n",
    "    w = w0.clone().detach()\n",
    "\n",
    "    # 数据分片\n",
    "    pieces = [(i*len(dataset)) // honestSize for i in range(honestSize+1)]\n",
    "    dataPerNode = [pieces[i+1] - pieces[i] for i in range(honestSize)]\n",
    "\n",
    "    lastGradients = torch.zeros_like(w0, dtype=torch.float64)\n",
    "\n",
    "    path = [F(w, dataset, weight_decay)]\n",
    "    variencePath = []\n",
    "    log('[SARAH]初始 loss={:.6f}, accuracy={:.2f} gamma={:}'.format(path[0], accuracy(w, dataset), gamma))\n",
    "    \n",
    "    # 中间变量分配空间\n",
    "    message = torch.zeros(nodeSize, len(w0), dtype=torch.float64)\n",
    "    newG = torch.zeros_like(w0, dtype=torch.float64)\n",
    "    lastw = torch.zeros_like(w0, dtype=torch.float64)\n",
    "    \n",
    "    # 随机的停止期限\n",
    "    randomStop = 1\n",
    "\n",
    "    log('开始迭代')\n",
    "    for r in range(rounds):\n",
    "        for k in range(displayInterval):\n",
    "            # snapshot\n",
    "            if (r*displayInterval + k) % randomStop == 0:\n",
    "                message.zero_()\n",
    "                for node in range(honestSize):\n",
    "                    for index in range(pieces[node], pieces[node+1]):\n",
    "                        x, y = dataset[index]\n",
    "                        predict = LogisticRegression(w, x)\n",
    "\n",
    "                        err = (predict-y).data\n",
    "                        message[node][:-1].add_(1/dataPerNode[node], err*x)\n",
    "                        message[node][-1].add_(1/dataPerNode[node], err)\n",
    "                    message[node].add_(weight_decay, w)\n",
    "                \n",
    "                # 首次更新\n",
    "                if attack != None:\n",
    "                    attack(message, byzantineSize)\n",
    "                g = aggregate(message)\n",
    "                lastw.copy_(w)\n",
    "                w.add_(-gamma, g)\n",
    "                # 指定下一次停止时间\n",
    "                randomStop = random.randint(1, snapshotInterval-1)\n",
    "            \n",
    "            # 诚实节点更新\n",
    "            for node in range(honestSize):\n",
    "                index = random.randint(pieces[node], pieces[node+1]-1)\n",
    "\n",
    "                x, y = dataset[index]\n",
    "                # 随机梯度\n",
    "                predict = LogisticRegression(w, x)\n",
    "                err = (predict-y).data\n",
    "                message[node][:-1].add_(err, x)\n",
    "                message[node][-1].add_(err, 1)\n",
    "                message[node].add_(weight_decay, w)\n",
    "                \n",
    "                # 修正梯度\n",
    "                predict = LogisticRegression(lastw, x)\n",
    "                err = (predict-y).data\n",
    "                message[node][:-1].add_(-err, x)\n",
    "                message[node][-1].add_(-err, 1)\n",
    "                message[node].add_(-weight_decay, lastw)\n",
    "\n",
    "            # 保存旧结果\n",
    "            lastw.copy_(w)\n",
    "            # 同步, Byzantine攻击\n",
    "            if attack != None:\n",
    "                attack(message, byzantineSize)\n",
    "            g = aggregate(message)\n",
    "            w.add_(-gamma, g)\n",
    "            \n",
    "        loss = F(w, dataset, weight_decay)\n",
    "        acc = accuracy(w, dataset)\n",
    "        path.append(loss)\n",
    "        var = getVarience(message, honestSize)\n",
    "        variencePath.append(var)\n",
    "        log('[SARAH]已迭代 {}/{} rounds (interval: {:.0f}), loss={:.9f}, accuracy={:.2f}, var={:.9f}'.format(\n",
    "            r+1, rounds, displayInterval, loss, acc, var\n",
    "        ))\n",
    "    return w, path, variencePath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 恶意攻击"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def white(messages, byzantinesize):\n",
    "    # 均值相同，方差较大\n",
    "    mu = torch.mean(messages[0:-byzantinesize], dim=0)\n",
    "    messages[-byzantinesize:].copy_(mu)\n",
    "    noise = torch.randn((byzantinesize, messages.size(1)), dtype=torch.float64)\n",
    "    messages[-byzantinesize:].add_(30, noise)\n",
    "def maxValue(messages, byzantinesize):\n",
    "    mu = torch.mean(messages[0:-byzantinesize], dim=0)\n",
    "    meliciousMessage = -3*mu\n",
    "    messages[-byzantinesize:].copy_(meliciousMessage)\n",
    "def zeroGradient(messages, byzantinesize):\n",
    "    s = torch.sum(messages[0:-byzantinesize], dim=0)\n",
    "    messages[-byzantinesize:].copy_(-s / byzantinesize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 驱动函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def run(optimizer, aggregate, attack, config):\n",
    "    \n",
    "    if attack == None:\n",
    "        title = '{}_{}_{}'.format(optimizer.__name__, 'baseline', aggregate.__name__)\n",
    "    else:\n",
    "        title = '{}_{}_{}'.format(optimizer.__name__, attack.__name__, aggregate.__name__)\n",
    "    print(dataSetConfig['name'] + '_' + title)\n",
    "    print('Fmin={}'.format(Fmin))\n",
    "\n",
    "    _VRConfig = config.copy()\n",
    "    _VRConfig['aggregate'] = aggregate\n",
    "    _VRConfig['attack'] = attack\n",
    "    if attack == None:\n",
    "        _VRConfig['byzantineSize'] = 0\n",
    "    w, path, variancePath = optimizer(w0, **_VRConfig)\n",
    "\n",
    "    record = {\n",
    "        **dataSetConfig,\n",
    "        'gamma': _VRConfig['gamma'],\n",
    "        'path': path,\n",
    "        'variancePath': variancePath,\n",
    "    }\n",
    "\n",
    "    with open(CACHE_DIR + title, 'wb') as f:\n",
    "        pickle.dump(record, f)\n",
    "\n",
    "    axis = plt.axes()\n",
    "    plt.plot(list(range(len(path))), logAxis(path, Fmin))\n",
    "    axis.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 运行实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 正确性测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "出现函数到达最小值后，重新回弹的现象，原因可能有\n",
    "1. 目标函数写错：忘记加惩罚项，忘记除以二等\n",
    "2. 触及机器精度边界"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 计算最小值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "从零开始跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_VRConfig = VRConfig.copy()\n",
    "_VRConfig['epoch'] = dataSetConfig['epoch'] * HONEST_SIZE * 2\n",
    "w_min = SAGA_min(w0, dataset, **_VRConfig)\n",
    "Fmin = F(w_min, dataset, _VRConfig['weight_decay'])\n",
    "print(Fmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "精度不够继续跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_VRConfig = VRConfig.copy()\n",
    "# _VRConfig['epoch'] = dataSetConfig['epoch'] * HONEST_SIZE\n",
    "_VRConfig['epoch'] = 20\n",
    "w_min = SAGA_min(w_min, dataset, **_VRConfig)\n",
    "Fmin = F(w_min, dataset, _VRConfig['weight_decay'])\n",
    "print(Fmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "存储Fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# with open(CACHE_DIR + 'Fmin', 'wb') as f:\n",
    "#     pickle.dump({\n",
    "#         'Fmin': Fmin,\n",
    "#         'w_min': w_min\n",
    "#     }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "读取Fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(CACHE_DIR + 'Fmin', 'rb') as f:\n",
    "    obj = pickle.load(f)\n",
    "    Fmin, w_min = obj['Fmin'], obj['w_min']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### SGD - mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = mean, attack = None, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = mean, attack = white, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = mean, attack = maxValue, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = mean, attack = zeroGradient, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### SGD - geomtric median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = gm, attack = None, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = gm, attack = white, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = gm, attack = maxValue, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = gm, attack = zeroGradient, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### SGD - Krum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Krum = Krum_(nodeSize=VRConfig['honestSize'], byzantineSize=0)\n",
    "run(optimizer = SGD, aggregate = Krum, attack = None, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Krum = Krum_(nodeSize=VRConfig['honestSize'], byzantineSize=VRConfig['byzantineSize'])\n",
    "run(optimizer = SGD, aggregate = Krum, attack = white, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Krum = Krum_(nodeSize=VRConfig['honestSize'], byzantineSize=VRConfig['byzantineSize'])\n",
    "run(optimizer = SGD, aggregate = Krum, attack = maxValue, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Krum = Krum_(nodeSize=VRConfig['honestSize'], byzantineSize=VRConfig['byzantineSize'])\n",
    "run(optimizer = SGD, aggregate = Krum, attack = zeroGradient, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### SGD - Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = median, attack = None, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = median, attack = white, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = median, attack = maxValue, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = median, attack = zeroGradient, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## BatchSGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### BatchSGD - mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log('Fmin={}'.format(Fmin))\n",
    "\n",
    "_VRConfig = batchConfig.copy()\n",
    "_VRConfig['aggregate'] = aggregate_linear\n",
    "_VRConfig['attack'] = None\n",
    "_VRConfig['byzantineSize'] = 0\n",
    "\n",
    "w, path, variancePath = FedBatchSGD(w0, **_VRConfig)\n",
    "\n",
    "record = {\n",
    "    **dataSetConfig,\n",
    "    'gamma': _VRConfig['gamma'],\n",
    "    'batchSize': _VRConfig['batchSize'],\n",
    "    'path': path,\n",
    "    'variancePath': variancePath,\n",
    "}\n",
    "\n",
    "with open(CACHE_DIR + 'BatchSGD_baseline_mean', 'wb') as f:\n",
    "    pickle.dump(record, f)\n",
    "    \n",
    "axis = plt.axes()\n",
    "plt.plot(list(range(len(path))), logAxis(path, Fmin))\n",
    "axis.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log('Fmin={}'.format(Fmin))\n",
    "\n",
    "_VRConfig = batchConfig.copy()\n",
    "_VRConfig['aggregate'] = aggregate_linear\n",
    "_VRConfig['attack'] = whiteNoise\n",
    "w, path, variancePath = FedBatchSGD(w0, **_VRConfig)\n",
    "\n",
    "record = {\n",
    "    **dataSetConfig,\n",
    "    'gamma': _VRConfig['gamma'],\n",
    "    'batchSize': _VRConfig['batchSize'],\n",
    "    'path': path,\n",
    "    'variancePath': variancePath,\n",
    "}\n",
    "\n",
    "with open(CACHE_DIR + 'BatchSGD_white_mean', 'wb') as f:\n",
    "    pickle.dump(record, f)\n",
    "    \n",
    "axis = plt.axes()\n",
    "plt.plot(list(range(len(path))), logAxis(path, Fmin))\n",
    "axis.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log('Fmin={}'.format(Fmin))\n",
    "\n",
    "_VRConfig = batchConfig.copy()\n",
    "_VRConfig['aggregate'] = aggregate_linear\n",
    "_VRConfig['attack'] = maxValue\n",
    "w, path, variancePath = FedBatchSGD(w0, **_VRConfig)\n",
    "\n",
    "record = {\n",
    "    **dataSetConfig,\n",
    "    'gamma': _VRConfig['gamma'],\n",
    "    'batchSize': _VRConfig['batchSize'],\n",
    "    'path': path,\n",
    "    'variancePath': variancePath,\n",
    "}\n",
    "\n",
    "with open(CACHE_DIR + 'BatchSGD_maxValue_mean', 'wb') as f:\n",
    "    pickle.dump(record, f)\n",
    "    \n",
    "axis = plt.axes()\n",
    "plt.plot(list(range(len(path))), logAxis(path, Fmin))\n",
    "axis.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log('Fmin={}'.format(Fmin))\n",
    "\n",
    "_VRConfig = batchConfig.copy()\n",
    "_VRConfig['aggregate'] = aggregate_linear\n",
    "_VRConfig['attack'] = zeroGradient\n",
    "w, path, variancePath = FedBatchSGD(w0, **_VRConfig)\n",
    "\n",
    "record = {\n",
    "    **dataSetConfig,\n",
    "    'gamma': _VRConfig['gamma'],\n",
    "    'batchSize': _VRConfig['batchSize'],\n",
    "    'path': path,\n",
    "    'variancePath': variancePath,\n",
    "}\n",
    "\n",
    "with open(CACHE_DIR + 'BatchSGD_zeroGradient_mean', 'wb') as f:\n",
    "    pickle.dump(record, f)\n",
    "    \n",
    "axis = plt.axes()\n",
    "plt.plot(list(range(len(path))), logAxis(path, Fmin))\n",
    "axis.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### BatchSGD - geomtric median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log('Fmin={}'.format(Fmin))\n",
    "\n",
    "_VRConfig = batchConfig.copy()\n",
    "_VRConfig['aggregate'] = aggregate_geometric\n",
    "_VRConfig['attack'] = None\n",
    "_VRConfig['byzantineSize'] = 0\n",
    "w, path, variancePath = FedBatchSGD(w0, **_VRConfig)\n",
    "\n",
    "record = {\n",
    "    **dataSetConfig,\n",
    "    'gamma': _VRConfig['gamma'],\n",
    "    'batchSize': _VRConfig['batchSize'],\n",
    "    'path': path,\n",
    "    'variancePath': variancePath,\n",
    "}\n",
    "\n",
    "with open(CACHE_DIR + 'BatchSGD_baseline_gm', 'wb') as f:\n",
    "    pickle.dump(record, f)\n",
    "    \n",
    "axis = plt.axes()\n",
    "plt.plot(list(range(len(path))), logAxis(path, Fmin))\n",
    "axis.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log('Fmin={}'.format(Fmin))\n",
    "\n",
    "_VRConfig = batchConfig.copy()\n",
    "_VRConfig['aggregate'] = aggregate_geometric\n",
    "_VRConfig['attack'] = whiteNoise\n",
    "w, path, variancePath = FedBatchSGD(w0, **_VRConfig)\n",
    "\n",
    "record = {\n",
    "    **dataSetConfig,\n",
    "    'gamma': _VRConfig['gamma'],\n",
    "    'batchSize': _VRConfig['batchSize'],\n",
    "    'path': path,\n",
    "    'variancePath': variancePath,\n",
    "}\n",
    "\n",
    "with open(CACHE_DIR + 'BatchSGD_white_gm', 'wb') as f:\n",
    "    pickle.dump(record, f)\n",
    "    \n",
    "axis = plt.axes()\n",
    "plt.plot(list(range(len(path))), logAxis(path, Fmin))\n",
    "axis.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log('Fmin={}'.format(Fmin))\n",
    "\n",
    "_VRConfig = batchConfig.copy()\n",
    "_VRConfig['aggregate'] = aggregate_geometric\n",
    "_VRConfig['attack'] = maxValue\n",
    "w, path, variancePath = FedBatchSGD(w0, **_VRConfig)\n",
    "\n",
    "record = {\n",
    "    **dataSetConfig,\n",
    "    'gamma': _VRConfig['gamma'],\n",
    "    'batchSize': _VRConfig['batchSize'],\n",
    "    'path': path,\n",
    "    'variancePath': variancePath,\n",
    "}\n",
    "\n",
    "with open(CACHE_DIR + 'BatchSGD_maxValue_gm', 'wb') as f:\n",
    "    pickle.dump(record, f)\n",
    "    \n",
    "axis = plt.axes()\n",
    "plt.plot(list(range(len(path))), logAxis(path, Fmin))\n",
    "axis.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log('Fmin={}'.format(Fmin))\n",
    "\n",
    "_VRConfig = batchConfig.copy()\n",
    "_VRConfig['aggregate'] = aggregate_geometric\n",
    "_VRConfig['attack'] = zeroGradient\n",
    "w, path, variancePath = FedBatchSGD(w0, **_VRConfig)\n",
    "\n",
    "record = {\n",
    "    **dataSetConfig,\n",
    "    'gamma': _VRConfig['gamma'],\n",
    "    'batchSize': _VRConfig['batchSize'],\n",
    "    'path': path,\n",
    "    'variancePath': variancePath,\n",
    "}\n",
    "\n",
    "with open(CACHE_DIR + 'BatchSGD_zeroGradient_gm', 'wb') as f:\n",
    "    pickle.dump(record, f)\n",
    "    \n",
    "axis = plt.axes()\n",
    "plt.plot(list(range(len(path))), logAxis(path, Fmin))\n",
    "axis.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## SAGA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### SAGA - mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = mean, attack = None, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = mean, attack = white, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = mean, attack = maxValue, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = mean, attack = zeroGradient, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### SAGA - geomtric median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = gm, attack = None, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = gm, attack = white, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = gm, attack = maxValue, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = gm, attack = zeroGradient, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### SAGA - Krum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Krum = Krum_(nodeSize=VRConfig['honestSize'], byzantineSize=0)\n",
    "run(optimizer = SAGA, aggregate = Krum, attack = None, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Krum = Krum_(nodeSize=VRConfig['honestSize'], byzantineSize=VRConfig['byzantineSize'])\n",
    "run(optimizer = SAGA, aggregate = Krum, attack = white, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Krum = Krum_(nodeSize=VRConfig['honestSize'], byzantineSize=VRConfig['byzantineSize'])\n",
    "run(optimizer = SAGA, aggregate = Krum, attack = maxValue, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Krum = Krum_(nodeSize=VRConfig['honestSize'], byzantineSize=VRConfig['byzantineSize'])\n",
    "run(optimizer = SAGA, aggregate = Krum, attack = zeroGradient, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### SAGA - Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = median, attack = None, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = median, attack = white, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = median, attack = maxValue, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = median, attack = zeroGradient, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## SVRG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### SVRG - mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "run(optimizer = SVRG, aggregate = mean, attack = None, config = SVRGConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "config = SVRGConfig.copy()\n",
    "config['displayInterval'] = dataSetConfig['dataSetSize']\n",
    "config['snapshotInterval'] = dataSetConfig['dataSetSize']\n",
    "run(optimizer = SVRG, aggregate = mean, attack = None, config = config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "上图拉长一点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "config = SVRGConfig.copy()\n",
    "config['displayInterval'] = int(dataSetConfig['dataSetSize']/4)\n",
    "config['snapshotInterval'] = dataSetConfig['dataSetSize']\n",
    "run(optimizer = SVRG, aggregate = mean, attack = None, config = config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "中心式 收敛效果应该很好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "config = SVRGConfig.copy()\n",
    "# config['displayInterval'] = dataSetConfig['dataSetSize']\n",
    "# config['snapshotInterval'] = dataSetConfig['dataSetSize']\n",
    "config['honestSize'] = 1\n",
    "run(optimizer = SVRG, aggregate = mean, attack = None, config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "config = SVRGConfig.copy()\n",
    "# config['displayInterval'] = dataSetConfig['dataSetSize']\n",
    "# config['snapshotInterval'] = dataSetConfig['dataSetSize']\n",
    "config['honestSize'] = 1\n",
    "run(optimizer = SVRG, aggregate = mean, attack = None, config = config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "为什么中心式的收敛效果还算那么差，调大步长也还是一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "config = SVRGConfig.copy()\n",
    "# config['displayInterval'] = dataSetConfig['dataSetSize']\n",
    "# config['snapshotInterval'] = dataSetConfig['dataSetSize']\n",
    "config['honestSize'] = 1\n",
    "config['gamma'] = 5e-2\n",
    "run(optimizer = SVRG, aggregate = mean, attack = None, config = config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "run(optimizer = SVRG, aggregate = mean, attack = whiteNoise, config = SVRGConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SVRG, aggregate = mean, attack = maxValue, config = SVRGConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SVRG, aggregate = mean, attack = zeroGradient, config = SVRGConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### SVRG - geomtric median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SVRG, aggregate = gm, attack = None, config = SVRGConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SVRG, aggregate = gm, attack = white, config = SVRGConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SVRG, aggregate = gm, attack = maxValue, config = SVRGConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SVRG, aggregate = gm, attack = zeroGradient, config = SVRGConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### SVRG - Krum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aggregate = Krum_(nodeSize=VRConfig['honestSize'], byzantineSize=0)\n",
    "run(optimizer = SVRG, aggregate = aggregate, attack = None, config = SVRGConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aggregate = Krum_(nodeSize=VRConfig['honestSize'], byzantineSize=VRConfig['byzantineSize'])\n",
    "run(optimizer = SVRG, aggregate = aggregate, attack = white, config = SVRGConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aggregate = Krum_(nodeSize=VRConfig['honestSize'], byzantineSize=VRConfig['byzantineSize'])\n",
    "run(optimizer = SVRG, aggregate = aggregate, attack = maxValue, config = SVRGConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aggregate = Krum_(nodeSize=VRConfig['honestSize'], byzantineSize=VRConfig['byzantineSize'])\n",
    "run(optimizer = SVRG, aggregate = aggregate, attack = zeroGradient, config = SVRGConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### SVRG - Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SVRG, aggregate = median, attack = None, config = SVRGConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SVRG, aggregate = median, attack = white, config = SVRGConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SVRG, aggregate = median, attack = maxValue, config = SVRGConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run(optimizer = SVRG, aggregate = median, attack = zeroGradient, config = SVRGConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## SARAH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### SARAH - mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "run(optimizer = SARAH, aggregate = mean, attack = None, config = SARAHConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "run(optimizer = SARAH, aggregate = mean, attack = white, config = SARAHConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SARAH, aggregate = mean, attack = maxValue, config = SARAHConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SARAH, aggregate = mean, attack = zeroGradient, config = SARAHConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### SARAH - geomtric median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SARAH, aggregate = gm, attack = None, config = SARAHConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SARAH, aggregate = gm, attack = white, config = SARAHConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SARAH, aggregate = gm, attack = maxValue, config = SARAHConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SARAH, aggregate = gm, attack = zeroGradient, config = SARAHConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### SARAH - Krum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aggregate = Krum_(nodeSize=VRConfig['honestNodeSize'], byzantineSize=0)\n",
    "run(optimizer = SARAH, aggregate = aggregate, attack = None, config = SARAHConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aggregate = Krum_(nodeSize=VRConfig['honestSize'], byzantineSize=VRConfig['byzantineSize'])\n",
    "run(optimizer = SARAH, aggregate = aggregate, attack = white, config = SARAHConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aggregate = Krum_(nodeSize=VRConfig['honestSize'], byzantineSize=VRConfig['byzantineSize'])\n",
    "run(optimizer = SARAH, aggregate = aggregate, attack = maxValue, config = SARAHConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aggregate = Krum_(nodeSize=VRConfig['honestSize'], byzantineSize=VRConfig['byzantineSize'])\n",
    "run(optimizer = SARAH, aggregate = aggregate, attack = zeroGradient, config = SARAHConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### SARAH - Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SARAH, aggregate = median, attack = None, config = SARAHConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SARAH, aggregate = median, attack = white, config = SARAHConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SARAH, aggregate = median, attack = maxValue, config = SARAHConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run(optimizer = SARAH, aggregate = median, attack = zeroGradient, config = SARAHConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 比较经过VR前后方差大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     37
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def FedSAGACmpVariance(w0, gamma, weight_decay, honestSize=0, byzantineSize=0, attack=None, \n",
    "            rounds=10, displayInterval=1000, SEED=100, fixSeed=False, **kw):\n",
    "    assert byzantineSize == 0 or attack != None\n",
    "    assert honestSize != 0\n",
    "    \n",
    "    if fixSeed:\n",
    "        random.seed(SEED)\n",
    "\n",
    "    nodeSize = honestSize + byzantineSize\n",
    "    \n",
    "    # 初始化\n",
    "    w = w0.clone().detach()\n",
    "\n",
    "    store = torch.zeros([len(dataset), w.size(0)], requires_grad=False, dtype=torch.float64)\n",
    "    for index in range(len(dataset)):\n",
    "        x, y = dataset[index]\n",
    "        predict = LogisticRegression(w, x)\n",
    "\n",
    "        err = (predict-y).data\n",
    "        store[index][:-1] = err*x\n",
    "        store[index][-1] = err\n",
    "        store[index].add_(weight_decay, w)\n",
    "\n",
    "    # 数据分片\n",
    "    pieces = [(i*len(dataset)) // honestSize for i in range(honestSize+1)]\n",
    "    dataPerNode = [pieces[i+1] - pieces[i] for i in range(honestSize)]\n",
    "\n",
    "    G_avg = torch.stack([\n",
    "        store[pieces[i]:pieces[i+1]].mean(dim=0) for i in range(honestSize)\n",
    "    ])\n",
    "    \n",
    "    # 返回函数值，SAGA和SGD对应的bias, variance, GM_error\n",
    "    path = [F(w, dataset, weight_decay)]\n",
    "    SAGA_biasPath = []\n",
    "    SAGA_variencePath = []\n",
    "    SAGA_error_Path = []\n",
    "    SGD_biasPath = []\n",
    "    SGD_variencePath = []\n",
    "    SGD_error_Path = []\n",
    "    \n",
    "    log('[SAGA]初始 loss={:.6f}, accuracy={:.2f} gamma={:}'.format(path[0], accuracy(w, dataset), gamma))\n",
    "    \n",
    "    # 中间变量分配空间\n",
    "    new_G = torch.zeros_like(w0, dtype=torch.float64)\n",
    "    message = torch.stack([w0] * nodeSize)\n",
    "    message_SGD = torch.stack([w0] * nodeSize)\n",
    "\n",
    "    log('开始迭代')\n",
    "    for r in range(rounds):\n",
    "        for k in range(displayInterval):\n",
    "            # 诚实节点更新\n",
    "            for node in range(honestSize):\n",
    "                index = random.randint(pieces[node], pieces[node+1]-1)\n",
    "\n",
    "                x, y = dataset[index]\n",
    "                # 更新梯度表\n",
    "                predict = LogisticRegression(w, x)\n",
    "\n",
    "                old_G = store[index]\n",
    "                err = (predict-y).data\n",
    "                new_G[:-1] = err*x\n",
    "                new_G[-1] = err\n",
    "                new_G.add_(weight_decay, w)\n",
    "\n",
    "                gradient = new_G.data - old_G.data + G_avg[node].data\n",
    "\n",
    "                G_avg[node].add_(1 / dataPerNode[node],\n",
    "                                 new_G.data - old_G.data)\n",
    "                store[index] = new_G.data\n",
    "                \n",
    "                message[node].copy_(gradient)\n",
    "                \n",
    "                if k + 1 == displayInterval:\n",
    "                    message_SGD[node].copy_(new_G)\n",
    "\n",
    "            # 同步\n",
    "            # Byzantine攻击\n",
    "            if attack != None:\n",
    "                attack(message, byzantineSize)\n",
    "            g = aggregate_geometric(message)\n",
    "            w.add_(-gamma, g.data)\n",
    "            \n",
    "        loss = F(w, dataset, weight_decay)\n",
    "        acc = accuracy(w, dataset)\n",
    "        \n",
    "        fullGradient = G(w, dataset, weight_decay)\n",
    "\n",
    "        # bias\n",
    "        SAGA_bias = message[:honestSize].mean(dim=0) - fullGradient\n",
    "        SAGA_biasPath.append((SAGA_bias.norm()).item())\n",
    "        SGD_bias = message_SGD[:honestSize].mean(dim=0) - fullGradient\n",
    "        SGD_biasPath.append((SGD_bias.norm()).item())\n",
    "\n",
    "        # varience\n",
    "        SAGA_var = getVarience(message, honestSize)\n",
    "        SAGA_variencePath.append(SAGA_var)\n",
    "        SGD_var = getVarience(message_SGD, honestSize)\n",
    "        SGD_variencePath.append(SGD_var)\n",
    "        # 函数值\n",
    "        path.append(loss)\n",
    "\n",
    "        # 记录error\n",
    "        SGD_gm_bias = aggregate_geometric(message_SGD) - fullGradient\n",
    "        SGD_error_Path.append(SGD_gm_bias.norm().item())\n",
    "        SAGA_gm_bias = aggregate_geometric(message) - fullGradient\n",
    "        SAGA_error_Path.append(SAGA_gm_bias.norm().item())\n",
    "        \n",
    "        log('[SAGA]已迭代 {}/{} rounds (interval: {:.0f}), loss={:.9f}, accuracy={:.2f}'.format(\n",
    "            r+1, rounds, displayInterval, loss, acc\n",
    "        ))\n",
    "        \n",
    "        print('[SGD] bias:{:.5f} var:{:.5f} err:{:.5f}'.format(SGD_biasPath[-1], SGD_variencePath[-1], SGD_error_Path[-1]))\n",
    "        print('[SAGA] bias:{:.5f} var:{:.5f} err:{:.5f}'.format(SAGA_biasPath[-1], SAGA_variencePath[-1], SAGA_error_Path[-1]))\n",
    "        \n",
    "    return w, path, SAGA_biasPath, SAGA_variencePath, SAGA_error_Path, SGD_biasPath, SGD_variencePath, SGD_error_Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log('Fmin={}'.format(Fmin))\n",
    "\n",
    "_VRConfig = VRConfig.copy()\n",
    "_VRConfig['attack'] = None\n",
    "_VRConfig['byzantineSize'] = 0\n",
    "res = FedSAGACmpVariance(w0, **_VRConfig)\n",
    "w, path, SAGA_biasPath, SAGA_variencePath, SAGA_error_Path, SGD_biasPath, SGD_variencePath, SGD_error_Path = res\n",
    "\n",
    "record = {\n",
    "    **dataSetConfig,\n",
    "    'gamma': _VRConfig['gamma'],\n",
    "    'path': path,\n",
    "    'SGD_biasPath': SGD_biasPath,\n",
    "    'SGD_variencePath': SGD_variencePath,\n",
    "    'SGD_error_Path': SGD_error_Path,\n",
    "    'SAGA_biasPath': SAGA_biasPath,\n",
    "    'SAGA_variencePath': SAGA_variencePath,\n",
    "    'SAGA_error_Path': SAGA_error_Path,\n",
    "}\n",
    "\n",
    "\n",
    "with open(CACHE_DIR + 'SGD_SAGA_cmpVar_baseline', 'wb') as f:\n",
    "    pickle.dump(record, f)\n",
    "    \n",
    "axis = plt.axes()\n",
    "plt.plot(list(range(len(path))), logAxis(path, Fmin))\n",
    "axis.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log('Fmin={}'.format(Fmin))\n",
    "\n",
    "_VRConfig = VRConfig.copy()\n",
    "_VRConfig['attack'] = whiteNoise\n",
    "res = FedSAGACmpVariance(w0, **_VRConfig)\n",
    "w, path, SAGA_biasPath, SAGA_variencePath, SAGA_error_Path, SGD_biasPath, SGD_variencePath, SGD_error_Path = res\n",
    "\n",
    "record = {\n",
    "    **dataSetConfig,\n",
    "    'gamma': _VRConfig['gamma'],\n",
    "    'path': path,\n",
    "    'SGD_biasPath': SGD_biasPath,\n",
    "    'SGD_variencePath': SGD_variencePath,\n",
    "    'SGD_error_Path': SGD_error_Path,\n",
    "    'SAGA_biasPath': SAGA_biasPath,\n",
    "    'SAGA_variencePath': SAGA_variencePath,\n",
    "    'SAGA_error_Path': SAGA_error_Path,\n",
    "}\n",
    "\n",
    "\n",
    "with open(CACHE_DIR + 'SGD_SAGA_cmpVar_white', 'wb') as f:\n",
    "    pickle.dump(record, f)\n",
    "    \n",
    "axis = plt.axes()\n",
    "plt.plot(list(range(len(path))), logAxis(path, Fmin))\n",
    "axis.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "maxValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log('Fmin={}'.format(Fmin))\n",
    "\n",
    "_VRConfig = VRConfig.copy()\n",
    "_VRConfig['attack'] = maxValue\n",
    "res = FedSAGACmpVariance(w0, **_VRConfig)\n",
    "w, path, SAGA_biasPath, SAGA_variencePath, SAGA_error_Path, SGD_biasPath, SGD_variencePath, SGD_error_Path = res\n",
    "\n",
    "record = {\n",
    "    **dataSetConfig,\n",
    "    'gamma': _VRConfig['gamma'],\n",
    "    'path': path,\n",
    "    'SGD_biasPath': SGD_biasPath,\n",
    "    'SGD_variencePath': SGD_variencePath,\n",
    "    'SGD_error_Path': SGD_error_Path,\n",
    "    'SAGA_biasPath': SAGA_biasPath,\n",
    "    'SAGA_variencePath': SAGA_variencePath,\n",
    "    'SAGA_error_Path': SAGA_error_Path,\n",
    "}\n",
    "\n",
    "\n",
    "with open(CACHE_DIR + 'SGD_SAGA_cmpVar_maxValue', 'wb') as f:\n",
    "    pickle.dump(record, f)\n",
    "    \n",
    "axis = plt.axes()\n",
    "plt.plot(list(range(len(path))), logAxis(path, Fmin))\n",
    "axis.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "zeroGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log('Fmin={}'.format(Fmin))\n",
    "\n",
    "_VRConfig = VRConfig.copy()\n",
    "_VRConfig['attack'] = zeroGradient\n",
    "res = FedSAGACmpVariance(w0, **_VRConfig)\n",
    "w, path, SAGA_biasPath, SAGA_variencePath, SAGA_error_Path, SGD_biasPath, SGD_variencePath, SGD_error_Path = res\n",
    "\n",
    "record = {\n",
    "    **dataSetConfig,\n",
    "    'gamma': _VRConfig['gamma'],\n",
    "    'path': path,\n",
    "    'SGD_biasPath': SGD_biasPath,\n",
    "    'SGD_variencePath': SGD_variencePath,\n",
    "    'SGD_error_Path': SGD_error_Path,\n",
    "    'SAGA_biasPath': SAGA_biasPath,\n",
    "    'SAGA_variencePath': SAGA_variencePath,\n",
    "    'SAGA_error_Path': SAGA_error_Path,\n",
    "}\n",
    "\n",
    "\n",
    "with open(CACHE_DIR + 'SGD_SAGA_cmpVar_zeroGradient', 'wb') as f:\n",
    "    pickle.dump(record, f)\n",
    "    \n",
    "axis = plt.axes()\n",
    "plt.plot(list(range(len(path))), logAxis(path, Fmin))\n",
    "axis.set_yscale('log')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "713px",
    "left": "1485px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
